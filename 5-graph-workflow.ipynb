{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "def converse_with_bedrock(model_client, sys_prompt, usr_prompt, model_id):\n",
    "    temperature = 0\n",
    "    top_p = 0.1\n",
    "    top_k = 1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = model_client.converse(\n",
    "        modelId=model_id, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "def create_prompt(sys_template, user_template, **kwargs):\n",
    "    sys_prompt = [{\"text\": sys_template.format(**kwargs)}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template.format(**kwargs)}]}]\n",
    "    return sys_prompt, usr_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from py2neo import Graph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Graph()\n",
    "region_name = \"us-west-2\"\n",
    "boto3_client = init_boto3_client(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional, Any\n",
    "\n",
    "class TraverseState(TypedDict, total=False):\n",
    "    parent_id: Optional[int]\n",
    "    parent_name: Optional[str]\n",
    "    child_level: int\n",
    "    selected_child_ids: List[int]\n",
    "    child_names: List[str]\n",
    "    next_action: str\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    high_priority_model: str\n",
    "    low_priority_model: str\n",
    "    region_name: str\n",
    "    question: str\n",
    "    subgraph: str\n",
    "    target_node: List[int]\n",
    "    next_step: str\n",
    "    parent_id: int\n",
    "    parent_name: str\n",
    "    content: str\n",
    "    contents_length: int\n",
    "    search_type: str\n",
    "    traverse_state: List[TraverseState]\n",
    "    searching_scheme: str\n",
    "    language: str\n",
    "    status: str\n",
    "    k: int\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgraph(state: GraphState) -> GraphState:\n",
    "    question = state['question']  \n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title {level: \"1\"})\n",
    "        RETURN n.value, id(n) as node_id\n",
    "    \"\"\"\n",
    "    results = graph.run(query)\n",
    "    subgraph_list = [(record[\"n.value\"], record[\"node_id\"]) for record in results]\n",
    "    subgraph_list_with_number = [f\"{i}. {subgraph[0]}\" for i, subgraph in enumerate(subgraph_list)]\n",
    "\n",
    "    sys_prompt_template = \"\"\" \n",
    "    You are an expert engineer well-versed in AWS manual documents. \n",
    "    Your task is to select the most appropriate manual document name for the user's question. \n",
    "    If there are no relevant documents, provide an empty list (\"\"). \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\" \n",
    "    Please select the single most relevant document name for the given question.\n",
    "\n",
    "    #Question: {question}\n",
    "\n",
    "    #Document List: {subgraph_list_with_number}\n",
    "\n",
    "    #Response Format: Provide only the index number of the selected document (omit any preamble) \"\"\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, subgraph_list_with_number=subgraph_list_with_number)\n",
    "    \n",
    "    model_id = state['low_priority_model']\n",
    "    selected_id = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "    print(selected_id)\n",
    "    try:\n",
    "        if selected_id == \"\":\n",
    "            return GraphState(target_node=[], subgraph=\"Not Found\", next_step=\"global_search\")\n",
    "\n",
    "        else: \n",
    "            selected_subgraph_id = subgraph_list[int(selected_id)][1]\n",
    "            print(\"Selected:\", subgraph_list[int(selected_id)][0])\n",
    "            return GraphState(target_node=[selected_subgraph_id], subgraph=subgraph_list[int(selected_id)][0], next_step=\"traverse_subgraph\")\n",
    "    except:\n",
    "        return GraphState(target_node=[], subgraph=\"Not Found\", next_step=\"global_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_subgraph(state: GraphState) -> GraphState:\n",
    "    question = state['question']\n",
    "    \n",
    "    if not state['traverse_state']:\n",
    "        parent_id = state['target_node'][0]\n",
    "    else:\n",
    "        parent_id = state['traverse_state'][-1]['selected_child_ids'][0]\n",
    "    subgraph = state['subgraph']\n",
    "\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE id(n) = $parent_id\n",
    "        OPTIONAL MATCH (n)-[:HAS_CHILD]->(c)\n",
    "        RETURN n.value as parent_name, c.level as child_level, c.value as child_name, id(c) as child_id\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    query_results = graph.run(query, params)\n",
    "\n",
    "    parent_name = None\n",
    "    child_level = None\n",
    "    child_list = []\n",
    "    child_names = []\n",
    "\n",
    "    for record in query_results:\n",
    "        if parent_name is None:\n",
    "            parent_name = record[\"parent_name\"]\n",
    "        if child_level is None:\n",
    "            child_level = record[\"child_level\"]\n",
    "        if record[\"child_name\"] is not None:\n",
    "            child_list.append((record[\"child_name\"], record[\"child_id\"]))\n",
    "            child_names.append(record[\"child_name\"])\n",
    "\n",
    "    print(f\"Traversing '{parent_name}'...\")\n",
    "\n",
    "    if not child_list:\n",
    "        print(\"No child. Proceed to 'get_contents'...\")\n",
    "        traverse_state: TraverseState = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"parent_name\": parent_name,\n",
    "            \"child_level\": -1,\n",
    "            \"selected_child_ids\": [],\n",
    "            \"child_names\": [],\n",
    "            \"next_action\": \"get_contents\"\n",
    "        }\n",
    "        return GraphState(traverse_state=state['traverse_state'] + [traverse_state])\n",
    "\n",
    "    child_list_with_number = [f\"{i}. {child}\" for i, child in enumerate(child_list)]\n",
    "    csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "    sys_prompt_template = \"\"\" You are an AI assistant specialized in AWS documentation, particularly for Bedrock services. Your task is to identify the most relevant sub-menu(s) from the provided <{subgraph}> manual to answer the user's question about AWS Bedrock.\n",
    "\n",
    "    Instructions:\n",
    "        1. Carefully analyze the given list of sub-menus.\n",
    "        2. Select the menu item(s) most directly related to the user's question.\n",
    "        3. Respond with the index number(s) of the selected menu item(s), starting from 0.\n",
    "\n",
    "    Selection criteria:\n",
    "        1. Prioritize menus that directly address the specific topic or feature mentioned in the question.\n",
    "        2. Look for keywords related to the question, such as \"custom models\", \"Bedrock\", \"implementation\", etc.\n",
    "        3. Prefer detailed, specific menu items over general or introductory ones.\n",
    "        4. If no menu item is sufficiently relevant, it's acceptable to not make a selection.\n",
    "\n",
    "    Important: Your response should ONLY include the index number(s) of the selected menu item(s), nothing else. \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\" \n",
    "    Question: {question}\n",
    "\n",
    "    Menu list: \n",
    "    {child_list_with_number}\n",
    "\n",
    "    Response format: {csv_list_response_format} \"\"\"\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, subgraph=subgraph, question=question, child_list_with_number=child_list_with_number, csv_list_response_format=csv_list_response_format)\n",
    "    model_id = state['high_priority_model']\n",
    "    selected_ids = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "    try:\n",
    "        selected_id_list = [int(id.strip()) for id in selected_ids.split(',') if id.strip().isdigit()]\n",
    "\n",
    "        if not selected_id_list:\n",
    "            traverse_state: TraverseState = {\n",
    "                \"parent_id\": parent_id,\n",
    "                \"parent_name\": parent_name,\n",
    "                \"child_level\": -1,\n",
    "                \"selected_child_ids\": [],\n",
    "                \"child_names\": [],\n",
    "                \"next_action\": \"get_contents\"\n",
    "            }\n",
    "            return GraphState(traverse_state=state['traverse_state'] + [traverse_state])\n",
    "\n",
    "        selected_child_ids = [child_list[id][1] for id in selected_id_list if id < len(child_list)]\n",
    "        selected_child_names = [child_list[id][0] for id in selected_id_list if id < len(child_list)]\n",
    "        traverse_state: TraverseState = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"parent_name\": parent_name,\n",
    "            \"child_level\": child_level,\n",
    "            \"selected_child_ids\": selected_child_ids,\n",
    "            \"child_names\": selected_child_names,\n",
    "            \"next_action\": \"traverse_subgraph\"\n",
    "        }\n",
    "        return GraphState(traverse_state=state['traverse_state'] + [traverse_state])\n",
    "\n",
    "    except Exception as e:\n",
    "        traverse_state: TraverseState = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"parent_name\": parent_name,\n",
    "            \"child_level\": -1,\n",
    "            \"selected_child_ids\": [],\n",
    "            \"child_names\": [],\n",
    "            \"next_action\": \"get_contents\"\n",
    "        }\n",
    "        return GraphState(traverse_state=state['traverse_state'] + [traverse_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(state: GraphState) -> GraphState:\n",
    "    parent_id = state['traverse_state'][-1]['parent_id']\n",
    "    k = state['k']\n",
    "    print(k)\n",
    "\n",
    "    count_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id\n",
    "        RETURN count(c) as contents_length, n.value as parent_name\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    count_result = graph.run(count_query, params).data()[0]\n",
    "    contents_length = count_result['contents_length']\n",
    "    parent_name = count_result['parent_name']\n",
    "    print(f\"Num Documents: {contents_length}\")\n",
    "\n",
    "    if contents_length <= k * 2:\n",
    "        search_type = \"get_short_documents\"\n",
    "        content_query = \"\"\"\n",
    "            MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "            WHERE id(n) = $parent_id\n",
    "            RETURN c.text\n",
    "            ORDER BY c.order\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "        params = {\"parent_id\": parent_id, \"k\": k}\n",
    "        content_results = graph.run(content_query, params)\n",
    "        contents = [record[\"c.text\"] for record in content_results]\n",
    "        context = \" \".join(contents)\n",
    "\n",
    "    else:\n",
    "        search_type = \"node_level_search\"\n",
    "        context = \"\"\n",
    "\n",
    "    new_state = state.copy()\n",
    "    new_state.update({\n",
    "        'parent_id': parent_id,\n",
    "        'parent_name': parent_name,\n",
    "        'content': context,\n",
    "        'contents_length': contents_length,\n",
    "        'search_type': search_type,\n",
    "        'k': k\n",
    "    })\n",
    "\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "\n",
    "def node_level_search(state: GraphState) -> GraphState:\n",
    "    question = state['question']  \n",
    "    subgraph = state['subgraph']\n",
    "    parent_id = state['parent_id']\n",
    "    parent_name = state['parent_name']\n",
    "    language = state['language']\n",
    "    searching_scheme = state['searching_scheme']\n",
    "    k = state['k']\n",
    "    \n",
    "    csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, please create a question suitable for vector search to find documents in the manual. ({language})\n",
    "\n",
    "        Note:\n",
    "        - Generate the most relevant and characteristic question considering both the document name and the user's question.\n",
    "        - Prefer natural language questions that well represent specific content of the document.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {subgraph}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = state['low_priority_model'] \n",
    "        keywords = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "        index_name = \"content_embedding_index\"\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=state['region_name'])\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "            MATCH (parent)-[:HAS_CONTENTS]->(child:Content)\n",
    "            WHERE id(parent) = $parent_id\n",
    "            WITH child\n",
    "            CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "            WHERE node = child\n",
    "            RETURN id(node) AS node_id, node.text AS text, score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "        \n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, extract one core keyword from the manual.\n",
    "        The keyword must meet the following conditions:\n",
    "        1. No special characters like '_', '-' in the keyword (e.g., respond with 'custom model' instead of 'custom_model')\n",
    "        2. Choose the most appropriate word within the given document name that fits the context of the question (no need to include the document name in the keyword)\n",
    "        3. Select specific words that represent the particular function or concept you're searching for, rather than content already included in the document name\n",
    "        4. Provide the keyword in {language}\n",
    "\n",
    "        Note:\n",
    "        - Consider both the document name and the question to select the most relevant and characteristic word within that document.\n",
    "        - Prefer words that represent specific content of the document rather than overly general terms.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {parent_name}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, parent_name=parent_name, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = state['low_priority_model'] \n",
    "        keywords = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "\n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = child\n",
    "                RETURN node.text as text, score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child, $keywords AS keyword\n",
    "                WHERE child.text CONTAINS keyword\n",
    "                RETURN child.text AS text, \n",
    "                    size(split(toLower(child.text), toLower(keyword))) - 1 AS score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"parent_id\": parent_id, \"keywords\": keywords, \"k\": k}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return GraphState(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevance(state: GraphState) -> GraphState:\n",
    "    question = state['question']\n",
    "    content = state['content']\n",
    "    contents_length = state['contents_length']\n",
    "    search_type = state['search_type']\n",
    "    parent_name = state['parent_name']\n",
    "    k = state['k']\n",
    "    optional_prompt1 = \"\"\n",
    "    optional_prompt2 = \"\"\n",
    "    \n",
    "    if search_type == \"get_short_documents\" and contents_length > k:\n",
    "        optional_prompt1 = \"- Preliminary information matches the question's intent but further content needs to be checked: 'Partial'\"\n",
    "        optional_prompt2 = \"or `Partial`\"\n",
    "\n",
    "    sys_prompt_template = \"\"\"\n",
    "    You are a skilled data analyst. Your task is to determine whether the given question can be answered using only the provided preliminary information, based on the following criteria:\n",
    "\n",
    "    Judgment criteria:\n",
    "    1. Do the key keywords of the question appear in the document name or preliminary information?\n",
    "    2. Does it contain the specific information we're trying to find out?\n",
    "\n",
    "    Response method:\n",
    "    - If the document name and preliminary information are irrelevant to the question: 'None'\n",
    "    - If the question can be answered with the preliminary information alone: 'Complete'\n",
    "    {partial1}\n",
    "\n",
    "    Skip any preamble and respond only with `None` or `Complete`{partial2}.\n",
    "    \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #Preliminary information (Document name: {parent_name})\n",
    "    {context}\n",
    "\n",
    "    #Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, partial1=optional_prompt1, partial2=optional_prompt2, parent_name=parent_name, question=question, context=content)\n",
    "    model_id = state['high_priority_model']\n",
    "    status = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "    return GraphState(status=status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling_contents(state: GraphState) -> GraphState:\n",
    "    \n",
    "    content = state['content']\n",
    "    parent_id = state['parent_id']\n",
    "    k = state['k']\n",
    "\n",
    "    content_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id AND c.order >= $order_pos\n",
    "        RETURN c.text\n",
    "        ORDER BY c.order\n",
    "        LIMIT $k\n",
    "    \"\"\"       \n",
    "    trial = 1\n",
    "    order_pos = k * trial\n",
    "    params = {\"parent_id\": parent_id, \"k\": k, \"order_pos\": order_pos} \n",
    "\n",
    "    content_results = graph.run(content_query, params)\n",
    "    sibling_content = [record[\"c.text\"] for record in content_results]\n",
    "    content = \" \".join([content] + sibling_content)\n",
    "    \n",
    "    return GraphState(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "def subgraph_level_search(state: GraphState) -> GraphState:\n",
    "    question = state['question']\n",
    "    \n",
    "    subgraph = state['subgraph']\n",
    "    language = state['language']\n",
    "    k = state['k']\n",
    "    searching_scheme = state['searching_scheme']\n",
    "\n",
    "    csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, please create a question suitable for vector search to find documents in the manual. ({language})\n",
    "\n",
    "        Note:\n",
    "        - Generate the most relevant and characteristic question considering both the document name and the user's question.\n",
    "        - Prefer natural language questions that well represent specific content of the document.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {subgraph}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = state['low_priority_model'] \n",
    "        keywords = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "        print(keywords)\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=state['region_name'])\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "        MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "        MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "\n",
    "        CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "        WHERE node = content\n",
    "\n",
    "        RETURN node.text AS text, score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"subgraph\": subgraph,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, extract one core keyword from the manual.\n",
    "        The keyword must meet the following conditions:\n",
    "        1. No special characters like '_', '-' in the keyword (e.g., respond with 'custom model' instead of 'custom_model')\n",
    "        2. Choose the most appropriate word within the given document name that fits the context of the question (no need to include the document name in the keyword)\n",
    "        3. Select specific words that represent the particular function or concept you're searching for, rather than content already included in the document name\n",
    "        4. Provide the keyword in {language}\n",
    "\n",
    "        Note:\n",
    "        - Consider both the document name and the question to select the most relevant and characteristic word within that document.\n",
    "        - Prefer words that represent specific content of the document rather than overly general terms.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {subgraph}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = state['low_priority_model'] \n",
    "        keywords = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "        \n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"\n",
    "                MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "                MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = content\n",
    "\n",
    "                RETURN node.text as text, score, title.name as title_name, title.level as title_level\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"\n",
    "                MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "                MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "                WITH content, title, $keywords AS keyword\n",
    "                WHERE content.text CONTAINS keyword\n",
    "                RETURN content.text AS text, \n",
    "                    size(split(toLower(content.text), toLower(keyword))) - 1 AS score,\n",
    "                    {\n",
    "                        title: title.name,\n",
    "                        level: title.level,\n",
    "                        value: title.value\n",
    "                    } AS metadata\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = { \"subgraph\": subgraph, \"k\": k, \"keywords\": keywords}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return GraphState(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "def global_search(state: GraphState) -> GraphState:\n",
    "    question = state['question']\n",
    "    \n",
    "    language = state['language']\n",
    "    searching_scheme = state['searching_scheme']\n",
    "    k = state['k']\n",
    "    csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "    \n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, please create a question suitable for vector search to find documents in the manual. ({language})\n",
    "\n",
    "        Note:\n",
    "        - Generate the most relevant and characteristic question considering both the document name and the user's question.\n",
    "        - Prefer natural language questions that well represent specific content of the document.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "    \n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = state['low_priority_model'] \n",
    "        keywords = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=state['region_name'])\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "        CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "        WITH DISTINCT node, score\n",
    "        WHERE node:Content\n",
    "        RETURN node.text AS text, score\n",
    "        ORDER BY score DESC\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, extract one core keyword from the manual.\n",
    "        The keyword must meet the following conditions:\n",
    "        1. No special characters like '_', '-' in the keyword (e.g., respond with 'custom model' instead of 'custom_model')\n",
    "        2. Choose the most appropriate word within the given document name that fits the context of the question (no need to include the document name in the keyword)\n",
    "        3. Select specific words that represent the particular function or concept you're searching for, rather than content already included in the document name\n",
    "        4. Provide the keyword in {language}\n",
    "\n",
    "        Note:\n",
    "        - Consider both the document name and the question to select the most relevant and characteristic word within that document.\n",
    "        - Prefer words that represent specific content of the document rather than overly general terms.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = state['low_priority_model'] \n",
    "        keywords = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "        print(keywords)\n",
    "        \n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query =\"\"\"\n",
    "            CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "            WHERE node:Content\n",
    "            OPTIONAL MATCH (title:Title)-[:HAS_CONTENTS]->(node)\n",
    "            RETURN node.text as text, score, title.name as title_name, title.level as title_level\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"\n",
    "            MATCH (content:Content)\n",
    "            WITH content, $keywords AS keyword\n",
    "            WHERE content.text CONTAINS keyword\n",
    "            OPTIONAL MATCH (title:Title)-[:HAS_CONTENTS]->(content)\n",
    "            RETURN content.text AS text, \n",
    "                size(split(toLower(content.text), toLower(keyword))) - 1 AS score,\n",
    "                {\n",
    "                    title: title.name,\n",
    "                    level: title.level,\n",
    "                    value: title.value\n",
    "                } AS metadata\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"k\": k, \"keywords\": keywords}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return GraphState(content = content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    question = state['question']\n",
    "    context = state['content']\n",
    "\n",
    "    # Prompt setting\n",
    "    sys_prompt_template = \"\"\"\n",
    "    You are an expert engineer well-versed in AWS. \n",
    "    Generate an answer to the user's question using only the given preliminary information. \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #Preliminary information: \n",
    "    {context}\n",
    "\n",
    "    #User question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, context=context, question=question)\n",
    "    model_id = state['high_priority_model'] \n",
    "    answer = converse_with_bedrock(boto3_client, sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "    return GraphState(answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_by_subgraph(state: GraphState) -> str:\n",
    "    return state['next_step']\n",
    "\n",
    "def next_step_by_traverse_state(state: GraphState) -> str:\n",
    "    if state['traverse_state'][-1]['next_action'] == \"get_contents\":\n",
    "        next_action = \"get_contents\"\n",
    "    else:\n",
    "        next_action = \"traverse_subgraph\"\n",
    "    return next_action\n",
    "\n",
    "def next_step_by_context(state: GraphState) -> str:\n",
    "    return state['search_type']\n",
    "\n",
    "def next_step_by_relevance(state: GraphState) -> str:\n",
    "    return state['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"select_subgraph\", select_subgraph)\n",
    "workflow.set_entry_point(\"select_subgraph\")\n",
    "workflow.add_node(\"traverse_subgraph\", traverse_subgraph)\n",
    "workflow.add_node(\"get_contents\", get_contents)\n",
    "workflow.add_node(\"node_level_search\", node_level_search)\n",
    "workflow.add_node(\"check_relevance\", check_relevance)\n",
    "workflow.add_node(\"get_sibling_contents\", get_sibling_contents)\n",
    "workflow.add_node(\"subgraph_level_search\", subgraph_level_search)\n",
    "workflow.add_node(\"global_search\", global_search)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"select_subgraph\",\n",
    "    next_step_by_subgraph,\n",
    "    {\n",
    "        \"global_search\": \"global_search\",\n",
    "        \"traverse_subgraph\": \"traverse_subgraph\",\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"traverse_subgraph\",\n",
    "    next_step_by_traverse_state,\n",
    "    {\n",
    "        \"get_contents\": \"get_contents\",\n",
    "        \"traverse_subgraph\": \"traverse_subgraph\",\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"get_contents\",\n",
    "    next_step_by_context,\n",
    "    {\n",
    "        \"get_short_documents\": \"check_relevance\",\n",
    "        \"node_level_search\": \"node_level_search\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"node_level_search\", \"check_relevance\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_relevance\",\n",
    "    next_step_by_relevance,\n",
    "    {\n",
    "        \"Complete\": \"generate_answer\",\n",
    "        \"Partial\": \"get_sibling_contents\",\n",
    "        \"None\": \"subgraph_level_search\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"get_sibling_contents\", \"generate_answer\")\n",
    "workflow.add_edge(\"subgraph_level_search\", \"generate_answer\")\n",
    "workflow.add_edge(\"global_search\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(app.get_graph(xray=True).draw_mermaid_png())\n",
    "    ) \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def run_query(question, high_priority_model, low_priority_model, region_name, searching_scheme, k, doc_language, app):\n",
    "    config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"TODO\"})\n",
    "    inputs = GraphState(\n",
    "        question=question,\n",
    "        high_priority_model=high_priority_model,\n",
    "        low_priority_model=low_priority_model,\n",
    "        region_name=region_name,\n",
    "        searching_scheme=searching_scheme,\n",
    "        k=k,\n",
    "        language=doc_language,\n",
    "        traverse_state=[]\n",
    "    )\n",
    "\n",
    "    pp = pprint.PrettyPrinter(width=200, compact=True)\n",
    "\n",
    "    try:\n",
    "        for output in app.stream(inputs, config=config):\n",
    "            for key, value in output.items():\n",
    "                print(f\"\\nüîπ [NODE] {key}\")\n",
    "                print(\"=\" * 80)\n",
    "                for k, v in value.items():\n",
    "                    print(f\"üìå {k}:\")\n",
    "                    pp.pprint(v)\n",
    "                print(\"=\" * 80)\n",
    "    except GraphRecursionError as e:\n",
    "        print(f\"‚ö†Ô∏è Recursion limit reached: {e}\")\n",
    "\n",
    "high_priority_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "#high_priority_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "#high_priority_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "low_priority_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "region_name = \"us-west-2\"\n",
    "searching_scheme = \"vector\"\n",
    "doc_language = \"English\"\n",
    "k = 5\n",
    "\n",
    "question = \"pricing policy in Amazon Bedrock\"\n",
    "run_query(question, high_priority_model, low_priority_model, region_name, searching_scheme, k, doc_language, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"The difference between Bedrock Studio and Bedrock\"\n",
    "run_query(question, high_priority_model, low_priority_model, region_name, searching_scheme, k, doc_language, app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

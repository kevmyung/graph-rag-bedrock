{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG Workflow Dev & Test\n",
    "\n",
    "![graph_rag_workflow](../images/graph_rag_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "#high_priority_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "#high_priority_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "high_priority_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "low_priority_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "def converse_with_bedrock(sys_prompt, usr_prompt, model_id):\n",
    "    temperature = 0\n",
    "    top_p = 0.1\n",
    "    top_k = 1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "def create_prompt(sys_template, user_template, **kwargs):\n",
    "    sys_prompt = [{\"text\": sys_template.format(**kwargs)}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template.format(**kwargs)}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "boto3_client = init_boto3_client(region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgraph_dev(question, graph):\n",
    "    question = question\n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title {level: \"1\"})\n",
    "        RETURN n.value, id(n) as node_id\n",
    "    \"\"\"\n",
    "    results = graph.run(query)\n",
    "    subgraph_list = [(record[\"n.value\"], record[\"node_id\"]) for record in results]\n",
    "    subgraph_list_with_number = [f\"{i}. {subgraph[0]}\" for i, subgraph in enumerate(subgraph_list)]\n",
    "\n",
    "    sys_prompt_template = \"\"\" \n",
    "    You are an expert engineer well-versed in AWS manual documents. \n",
    "    Your task is to select the most appropriate manual document name for the user's question. \n",
    "    If there are no relevant documents, provide an empty list (\"\"). \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\" \n",
    "    Please select the single most relevant document name for the given question.\n",
    "\n",
    "    #Question: {question}\n",
    "\n",
    "    #Document List: {subgraph_list_with_number}\n",
    "\n",
    "    #Response Format: Provide only the index number of the selected document (omit any preamble) \"\"\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, subgraph_list_with_number=subgraph_list_with_number)\n",
    "    \n",
    "    model_id = low_priority_model \n",
    "    selected_id = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    try:\n",
    "        if selected_id == \"\":\n",
    "            return [], \"\", \"generate_answer\"\n",
    "\n",
    "        else: \n",
    "            selected_subgraph_id = subgraph_list[int(selected_id)][1]\n",
    "            print(\"Selected:\", subgraph_list[int(selected_id)][0])\n",
    "            return [selected_subgraph_id], subgraph_list[int(selected_id)][0], \"traverse_child\"\n",
    "    except:\n",
    "        return [], \"\", \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"available model list in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택 (현재 없는 내용)\n",
    "question = \"how to deploy a model in SageMaker\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraverseResult:\n",
    "    def __init__(self, parent_id, parent_name, child_level, selected_child_ids, child_names, next_action):\n",
    "        self.parent_id = parent_id\n",
    "        self.parent_name = parent_name\n",
    "        self.child_level = child_level\n",
    "        self.selected_child_ids = selected_child_ids\n",
    "        self.child_names = child_names\n",
    "        self.next_action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def traverse_child_dev(question, subgraph, graph, target_node):\n",
    "    parent_id = target_node[0]\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE id(n) = $parent_id\n",
    "        OPTIONAL MATCH (n)-[:HAS_CHILD]->(c)\n",
    "        RETURN n.value as parent_name, c.level as child_level, c.value as child_name, id(c) as child_id\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    query_results = graph.run(query, params)\n",
    "\n",
    "    parent_name = None\n",
    "    child_level = None\n",
    "    child_list = []\n",
    "    child_names = []\n",
    "\n",
    "    for record in query_results:\n",
    "        if parent_name is None:\n",
    "            parent_name = record[\"parent_name\"]\n",
    "        if child_level is None:\n",
    "            child_level = record[\"child_level\"]\n",
    "        if record[\"child_name\"] is not None:\n",
    "            child_list.append((record[\"child_name\"], record[\"child_id\"]))\n",
    "            child_names.append(record[\"child_name\"])\n",
    "\n",
    "    print(f\"Traversing '{parent_name}'...\")\n",
    "\n",
    "    if not child_list:\n",
    "        print(\"No child. Proceed to 'get_contents'...\")\n",
    "        #print(f\"Debug: {parent_id}, {parent_name}, {child_level}, [], [], 'get_contents'\")\n",
    "        return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")\n",
    "\n",
    "    child_list_with_number = [f\"{i}. {child}\" for i, child in enumerate(child_list)]\n",
    "    sys_prompt_template = \"\"\" You are an AI assistant specialized in AWS documentation, particularly for Bedrock services. Your task is to identify the most relevant sub-menu(s) from the provided <{subgraph}> manual to answer the user's question about AWS Bedrock.\n",
    "\n",
    "    Instructions:\n",
    "        1. Carefully analyze the given list of sub-menus.\n",
    "        2. Select the menu item(s) most directly related to the user's question.\n",
    "        3. Respond with the index number(s) of the selected menu item(s), starting from 0.\n",
    "\n",
    "    Selection criteria:\n",
    "        1. Prioritize menus that directly address the specific topic or feature mentioned in the question.\n",
    "        2. Look for keywords related to the question, such as \"custom models\", \"Bedrock\", \"implementation\", etc.\n",
    "        3. Prefer detailed, specific menu items over general or introductory ones.\n",
    "        4. If no menu item is sufficiently relevant, it's acceptable to not make a selection.\n",
    "\n",
    "    Important: Your response should ONLY include the index number(s) of the selected menu item(s), nothing else. \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\" \n",
    "    Question: {question}\n",
    "\n",
    "    Menu list: \n",
    "    {child_list_with_number}\n",
    "\n",
    "    Response format: {csv_list_response_format} \"\"\"\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, subgraph=subgraph, question=question, child_list_with_number=child_list_with_number, csv_list_response_format=csv_list_response_format)\n",
    "    model_id = high_priority_model\n",
    "    selected_ids = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "    try:\n",
    "        selected_id_list = [int(id.strip()) for id in selected_ids.split(',') if id.strip().isdigit()]\n",
    "\n",
    "        if not selected_id_list:\n",
    "            #print(f\"Debug1: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "            return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")\n",
    "        \n",
    "        selected_child_ids = [child_list[id][1] for id in selected_id_list if id < len(child_list)]\n",
    "        selected_child_names = [child_list[id][0] for id in selected_id_list if id < len(child_list)]\n",
    "\n",
    "        #print(f\"Debug2: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "        return TraverseResult(parent_id, parent_name, child_level, selected_child_ids, selected_child_names, \"traverse_child\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        #print(f\"Debug3: Exception occurred: {str(e)}\")\n",
    "        #print(f\"Debug4: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "        return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, parent_id, parent_name, contents, contents_length, search_type, k):\n",
    "        self.parent_id = parent_id\n",
    "        self.parent_name = parent_name\n",
    "        self.contents = contents\n",
    "        self.contents_length = contents_length\n",
    "        self.search_type = search_type\n",
    "        self.k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traverse_results = []\n",
    "\n",
    "# 질문의 주제 선택\n",
    "question = \"how to utilize custom models in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_dev(graph, parent_id, k=5):\n",
    "    count_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id\n",
    "        RETURN count(c) as contents_length, n.value as parent_name\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    count_result = graph.run(count_query, params).data()[0]\n",
    "    contents_length = count_result['contents_length']\n",
    "    parent_name = count_result['parent_name']\n",
    "    print(f\"Num Documents: {contents_length}\")\n",
    "\n",
    "    if contents_length <= k * 2:\n",
    "        search_type = \"get_short_documents\"\n",
    "        content_query = \"\"\"\n",
    "            MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "            WHERE id(n) = $parent_id\n",
    "            RETURN c.text\n",
    "            ORDER BY c.order\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "        params = {\"parent_id\": parent_id, \"k\": k}\n",
    "        content_results = graph.run(content_query, params)\n",
    "        contents = [record[\"c.text\"] for record in content_results]\n",
    "        context = \" \".join(contents)\n",
    "\n",
    "    else:\n",
    "        search_type = \"node_level_search\"\n",
    "        context = \"\"\n",
    "\n",
    "    return Context(parent_id, parent_name, context, contents_length, search_type, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"available model list provided in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to define Agent Actions using SDK in Bedrock's Agent\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"full_text\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def node_level_search_dev(question, graph, subgraph, parent_id, parent_name, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, please create a question suitable for vector search to find documents in the manual. ({language})\n",
    "\n",
    "        Note:\n",
    "        - Generate the most relevant and characteristic question considering both the document name and the user's question.\n",
    "        - Prefer natural language questions that well represent specific content of the document.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {subgraph}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_priority_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "        index_name = \"content_embedding_index\"\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=region_name)\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "            MATCH (parent)-[:HAS_CONTENTS]->(child:Content)\n",
    "            WHERE id(parent) = $parent_id\n",
    "            WITH child\n",
    "            CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "            WHERE node = child\n",
    "            RETURN id(node) AS node_id, node.text AS text, score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "        \n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, extract one core keyword from the manual.\n",
    "        The keyword must meet the following conditions:\n",
    "        1. No special characters like '_', '-' in the keyword (e.g., respond with 'custom model' instead of 'custom_model')\n",
    "        2. Choose the most appropriate word within the given document name that fits the context of the question (no need to include the document name in the keyword)\n",
    "        3. Select specific words that represent the particular function or concept you're searching for, rather than content already included in the document name\n",
    "        4. Provide the keyword in {language}\n",
    "\n",
    "        Note:\n",
    "        - Consider both the document name and the question to select the most relevant and characteristic word within that document.\n",
    "        - Prefer words that represent specific content of the document rather than overly general terms.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {parent_name}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, parent_name=parent_name, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_priority_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "\n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = child\n",
    "                RETURN node.text as text, score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child, $keywords AS keyword\n",
    "                WHERE child.text CONTAINS keyword\n",
    "                RETURN child.text AS text, \n",
    "                    size(split(toLower(child.text), toLower(keyword))) - 1 AS score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"parent_id\": parent_id, \"keywords\": keywords, \"k\": k}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to define Agent Actions using SDK in Bedrock's Agent\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    search_content = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "    print(search_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevance_dev(question, content, parent_name, contents_length, search_type, k=5):\n",
    "    optional_prompt1 = \"\"\n",
    "    optional_prompt2 = \"\"\n",
    "    \n",
    "    if search_type == \"get_short_documents\" and contents_length > k:\n",
    "        optional_prompt1 = \"- Preliminary information matches the question's intent but further content needs to be checked: 'Partial'\"\n",
    "        optional_prompt2 = \"or `Partial`\"\n",
    "\n",
    "    sys_prompt_template = \"\"\"\n",
    "    You are a skilled data analyst. Your task is to determine whether the given question can be answered using only the provided preliminary information, based on the following criteria:\n",
    "\n",
    "    Judgment criteria:\n",
    "    1. Do the key keywords of the question appear in the document name or preliminary information?\n",
    "    2. Does it contain the specific information we're trying to find out?\n",
    "\n",
    "    Response method:\n",
    "    - If the document name and preliminary information are irrelevant to the question: 'None'\n",
    "    - If the question can be answered with the preliminary information alone: 'Complete'\n",
    "    {partial1}\n",
    "\n",
    "    Skip any preamble and respond only with `None` or `Complete`{partial2}.\n",
    "    \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #Preliminary information (Document name: {parent_name})\n",
    "    {context}\n",
    "\n",
    "    #Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, partial1=optional_prompt1, partial2=optional_prompt2, parent_name=parent_name, question=question, context=content)\n",
    "    model_id = high_priority_model\n",
    "    status = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to encrypt data in model customization tasks\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to utilize custom models in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to utilize the memory feature in Bedrock Agent\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling_contents_dev(graph, parent_id, content, k=5):\n",
    "    content_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id AND c.order >= $order_pos\n",
    "        RETURN c.text\n",
    "        ORDER BY c.order\n",
    "        LIMIT $k\n",
    "    \"\"\"       \n",
    "    trial = 1\n",
    "    order_pos = k * trial\n",
    "    params = {\"parent_id\": parent_id, \"k\": k, \"order_pos\": order_pos} \n",
    "\n",
    "    content_results = graph.run(content_query, params)\n",
    "    sibling_content = [record[\"c.text\"] for record in content_results]\n",
    "    contents = \" \".join([content] + sibling_content)\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"web pages as a data source for Knowledge Base in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택 \n",
    "question = \"Response format when using the Converse API in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Found\n",
    "question = \"pricing policy in Amazon Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"full_text\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def subgraph_level_search_dev(question, graph, subgraph, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        print(\"vector search started\")\n",
    "\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, please create a question suitable for vector search to find documents in the manual. ({language})\n",
    "\n",
    "        Note:\n",
    "        - Generate the most relevant and characteristic question considering both the document name and the user's question.\n",
    "        - Prefer natural language questions that well represent specific content of the document.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {subgraph}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_priority_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "        print(keywords)\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=region_name)\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "        MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "        MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "\n",
    "        CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "        WHERE node = content\n",
    "\n",
    "        RETURN node.text AS text, score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"subgraph\": subgraph,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        print(\"text search started\")\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, extract one core keyword from the manual.\n",
    "        The keyword must meet the following conditions:\n",
    "        1. No special characters like '_', '-' in the keyword (e.g., respond with 'custom model' instead of 'custom_model')\n",
    "        2. Choose the most appropriate word within the given document name that fits the context of the question (no need to include the document name in the keyword)\n",
    "        3. Select specific words that represent the particular function or concept you're searching for, rather than content already included in the document name\n",
    "        4. Provide the keyword in {language}\n",
    "\n",
    "        Note:\n",
    "        - Consider both the document name and the question to select the most relevant and characteristic word within that document.\n",
    "        - Prefer words that represent specific content of the document rather than overly general terms.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Target document name:\n",
    "        {subgraph}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_priority_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "        \n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"\n",
    "                MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "                MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = content\n",
    "\n",
    "                RETURN node.text as text, score, title.name as title_name, title.level as title_level\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"\n",
    "                MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "                MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "                WITH content, title, $keywords AS keyword\n",
    "                WHERE content.text CONTAINS keyword\n",
    "                RETURN content.text AS text, \n",
    "                    size(split(toLower(content.text), toLower(keyword))) - 1 AS score,\n",
    "                    {\n",
    "                        title: title.name,\n",
    "                        level: title.level,\n",
    "                        value: title.value\n",
    "                    } AS metadata\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = { \"subgraph\": subgraph, \"k\": k, \"keywords\": keywords}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Found\n",
    "question = \"pricing policy in Amazon Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "print(context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"vector\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def global_search_dev(question, graph, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, please create a question suitable for vector search to find documents in the manual. ({language})\n",
    "\n",
    "        Note:\n",
    "        - Generate the most relevant and characteristic question considering both the document name and the user's question.\n",
    "        - Prefer natural language questions that well represent specific content of the document.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_priority_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=region_name)\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "        CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "        WITH DISTINCT node, score\n",
    "        WHERE node:Content\n",
    "        RETURN node.text AS text, score\n",
    "        ORDER BY score DESC\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        You are an expert engineer well-versed in AWS. Based on the user's question, extract one core keyword from the manual.\n",
    "        The keyword must meet the following conditions:\n",
    "        1. No special characters like '_', '-' in the keyword (e.g., respond with 'custom model' instead of 'custom_model')\n",
    "        2. Choose the most appropriate word within the given document name that fits the context of the question (no need to include the document name in the keyword)\n",
    "        3. Select specific words that represent the particular function or concept you're searching for, rather than content already included in the document name\n",
    "        4. Provide the keyword in {language}\n",
    "\n",
    "        Note:\n",
    "        - Consider both the document name and the question to select the most relevant and characteristic word within that document.\n",
    "        - Prefer words that represent specific content of the document rather than overly general terms.\n",
    "        \"\"\"\n",
    "\n",
    "        usr_prompt_template = \"\"\"\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Response format:\n",
    "        {csv_list_response_format}\n",
    "        \"\"\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_priority_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "        print(keywords)\n",
    "        \n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query =\"\"\"\n",
    "            CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "            WHERE node:Content\n",
    "            OPTIONAL MATCH (title:Title)-[:HAS_CONTENTS]->(node)\n",
    "            RETURN node.text as text, score, title.name as title_name, title.level as title_level\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"\n",
    "            MATCH (content:Content)\n",
    "            WITH content, $keywords AS keyword\n",
    "            WHERE content.text CONTAINS keyword\n",
    "            OPTIONAL MATCH (title:Title)-[:HAS_CONTENTS]->(content)\n",
    "            RETURN content.text AS text, \n",
    "                size(split(toLower(content.text), toLower(keyword))) - 1 AS score,\n",
    "                {\n",
    "                    title: title.name,\n",
    "                    level: title.level,\n",
    "                    value: title.value\n",
    "                } AS metadata\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"k\": k, \"keywords\": keywords}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"pricing policy in Amazon Bedrock\"\n",
    "\n",
    "content = global_search_dev(question, graph, 'English', 5)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate_answer_dev(question, context):\n",
    "    # Prompt setting\n",
    "    sys_prompt_template = \"\"\"\n",
    "    You are an expert engineer well-versed in AWS. Generate an answer to the user's question using only the given preliminary information. If the question asks about content not provided in the preliminary information, respond that you don't know.\"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #Preliminary information: \n",
    "    {context}\n",
    "\n",
    "    #User question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", sys_prompt_template), (\"human\",usr_prompt_template)])\n",
    "\n",
    "    # Model setting\n",
    "    model_kwargs = {\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    llm = ChatBedrock(model_id=high_priority_model, region_name=\"us-west-2\", model_kwargs=model_kwargs, streaming=True)   \n",
    "\n",
    "    # Output setting\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | parser\n",
    "    for chunk in chain.stream({\"context\": context, \"question\": question}):\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to optimize the performance in Bedrock's Agent\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to evaluate Knowledge Base in Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"pricing policy for Bedrock\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Amazon Bedrock Playground?\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, subgraph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

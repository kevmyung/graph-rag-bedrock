{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서를 Tree Graph 형태로 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 후처리\n",
    "\n",
    "1. 코드 주석의 `#`를 `//`로 변환\n",
    "2. 목차의 `.....` 삭제\n",
    "3. 페이지 구분자 `-----` 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\" #\", \" //\")\n",
    "    text = text.replace(\".....\", \"\")\n",
    "    text = text.replace(\"-----\", \"\")\n",
    "    text = text.replace(\"**\", \"\")\n",
    "    text = text.replace(\"```\", \"---\") \n",
    "    return text\n",
    "\n",
    "with open(\"samples/bedrock-manual.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    md_text_origin = file.read()\n",
    "\n",
    "md_text = clean_text(md_text_origin)\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header 단위로 1차 텍스트 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"1\"),\n",
    "    (\"##\", \"2\"),\n",
    "    (\"###\", \"3\"),\n",
    "    (\"####\", \"4\"),\n",
    "    (\"#####\", \"5\")\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(md_text)\n",
    "\n",
    "for header in md_header_splits:\n",
    "    content_length = len(header.page_content)\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "def custom_text_splitter(text: str) -> List[str]:\n",
    "    def split_chunk(chunk: str, delimiters: List[str]) -> List[str]:\n",
    "        for delimiter in delimiters:\n",
    "            if delimiter in chunk:\n",
    "                sub_chunks = [sc.strip() for sc in chunk.split(delimiter) if sc.strip()]\n",
    "                if all(len(sc) >= 300 for sc in sub_chunks):\n",
    "                    return sub_chunks\n",
    "        return [chunk]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    delimiters = [\"\\n\\n\", \"\\n---\\n\", \"\\n\", \" \"]\n",
    "\n",
    "    for paragraph in text.split(\"\\n\\n\"):\n",
    "        if len(current_chunk) + len(paragraph) + 2 <= 1000:\n",
    "            current_chunk += (\"\\n\\n\" + paragraph) if current_chunk else paragraph\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.extend(split_chunk(current_chunk, delimiters))\n",
    "            current_chunk = paragraph\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.extend(split_chunk(current_chunk, delimiters))\n",
    "\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) < 300:\n",
    "            if final_chunks and len(final_chunks[-1]) + len(chunk) + 2 <= 1000:\n",
    "                final_chunks[-1] += \"\\n\\n\" + chunk\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        elif len(chunk) > 1000:\n",
    "            words = chunk.split()\n",
    "            temp_chunk = \"\"\n",
    "            for word in words:\n",
    "                if len(temp_chunk) + len(word) + 1 <= 1000:\n",
    "                    temp_chunk += \" \" + word if temp_chunk else word\n",
    "                else:\n",
    "                    if len(temp_chunk) >= 300:\n",
    "                        final_chunks.append(temp_chunk)\n",
    "                        temp_chunk = word\n",
    "                    else:\n",
    "                        temp_chunk += \" \" + word\n",
    "            if temp_chunk:\n",
    "                final_chunks.append(temp_chunk)\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def preprocess_content(content: str, vector_search_enabled: bool) -> List[dict]:\n",
    "    processed_splits = []\n",
    "    embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=\"us-east-1\")\n",
    "    order = 0\n",
    "\n",
    "    chunks = custom_text_splitter(content)\n",
    "    for chunk in chunks:\n",
    "        embedding = embeddings.embed_query(chunk) if vector_search_enabled else []\n",
    "        processed_splits.append({\n",
    "            'text': chunk,\n",
    "            'embedding': embedding,\n",
    "            'order': order\n",
    "        })\n",
    "        order += 1\n",
    "\n",
    "        # 길이가 1000보다 큰 chunk 출력\n",
    "        if len(chunk) > 1000:\n",
    "            print(f\"Large chunk detected (length: {len(chunk)}):\")\n",
    "            print(chunk)\n",
    "            print(\"-\" * 50)  \n",
    "\n",
    "    return processed_splits\n",
    "\n",
    "# 테스트\n",
    "vector_search_enabled = False\n",
    "for doc in md_header_splits:\n",
    "    preprocessed_contents = preprocess_content(doc.page_content, vector_search_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title 노드와 Content 노드로 분리 & Content를 길이에 따라 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "from langchain.schema import Document\n",
    "\n",
    "def create_graph_document_from_md_splits(md_header_splits: List[Document], vector_search_enabled: bool) -> List[GraphDocument]:\n",
    "    nodes = {}\n",
    "    relationships = []\n",
    "    contents = []\n",
    "\n",
    "    for doc in md_header_splits:\n",
    "        preprocessed_contents = preprocess_content(doc.page_content, vector_search_enabled)\n",
    "\n",
    "        headers = sorted([(k, v) for k, v in doc.metadata.items() if k.split()[0].isdigit() and v],\n",
    "                        key=lambda x: int(x[0]))\n",
    "\n",
    "        parent_node = None\n",
    "        for i, (header_key, header_value) in enumerate(headers):\n",
    "            node_id = f\"{header_key}_{header_value}\"\n",
    "            if node_id not in nodes:\n",
    "                nodes[node_id] = Node(id=node_id, type='Title', properties={'level': header_key, 'value': header_value})\n",
    "\n",
    "            if parent_node:\n",
    "                relationships.append(Relationship(\n",
    "                    source=nodes[parent_node],\n",
    "                    target=nodes[node_id],\n",
    "                    type='HAS_CHILD'\n",
    "                ))\n",
    "\n",
    "            parent_node = node_id\n",
    "\n",
    "        # Add content relationships for each preprocessed content\n",
    "        for processed_content in preprocessed_contents:\n",
    "            content_node_id = f\"{len(contents)}_{processed_content['order']}\"\n",
    "            content_node = Node(id=content_node_id, type='Content', properties={\n",
    "                'text': processed_content['text'],\n",
    "                'embedding': processed_content['embedding'],\n",
    "                'order': processed_content['order'] \n",
    "            })\n",
    "            nodes[content_node_id] = content_node\n",
    "            contents.append(content_node)\n",
    "\n",
    "            if parent_node:\n",
    "                relationships.append(Relationship(\n",
    "                    source=nodes[parent_node],\n",
    "                    target=content_node,\n",
    "                    type='HAS_CONTENTS'\n",
    "                ))\n",
    "            else:\n",
    "                # If there's no header, create a default one\n",
    "                default_header_id = f\"_{len(nodes)}\"\n",
    "                default_header = Node(id=default_header_id, type='Title', properties={'level': '', 'value': 'Default Header'})\n",
    "                nodes[default_header_id] = default_header\n",
    "                relationships.append(Relationship(\n",
    "                    source=default_header,\n",
    "                    target=content_node,\n",
    "                    type='HAS_CONTENTS'\n",
    "                ))\n",
    "\n",
    "    source_doc = Document(page_content=\"Bedrock Graph\")\n",
    "    graph_doc = GraphDocument(\n",
    "        nodes=list(nodes.values()),\n",
    "        relationships=relationships,\n",
    "        source=source_doc\n",
    "    )\n",
    "\n",
    "    return [graph_doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 생성\n",
    "\n",
    "| `vector_search_enabled` | 소요 시간 |\n",
    "|:---|:---|\n",
    "| `True` | ~30분 소요 |\n",
    "| `False` | 1~2분 소요 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_enabled=True\n",
    "graph_docs = create_graph_document_from_md_splits(md_header_splits, vector_search_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중간 처리 된 그래프 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('samples/graph_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(graph_docs, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

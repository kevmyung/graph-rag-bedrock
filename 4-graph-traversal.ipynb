{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /home/ubuntu/.local/lib/python3.10/site-packages (0.0.55)\n",
      "Requirement already satisfied: uuid6<2025.0.0,>=2024.1.12 in /home/ubuntu/.local/lib/python3.10/site-packages (from langgraph) (2024.1.12)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langgraph) (0.2.34)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.85)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.8.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.32.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.10.6)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region_name = \"us-east-1\"\n",
    "#llm_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "llm_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "def converse_with_bedrock(sys_prompt, usr_prompt):\n",
    "    temperature = 0.1\n",
    "    top_p = 0.3\n",
    "    top_k = 10\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=llm_model, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "\n",
    "def create_prompt(sys_template, user_template, **kwargs):\n",
    "    sys_prompt = [{\"text\": sys_template.format(**kwargs)}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template.format(**kwargs)}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "boto3_client = init_boto3_client(region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    question: str\n",
    "    subgraph: str\n",
    "    target_node: List[int]\n",
    "    next_step: str\n",
    "    node_pos: Optional[int]\n",
    "    trace: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def select_subgraph(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title {level: \"1\"})\n",
    "        RETURN n.value, id(n) as node_id\n",
    "    \"\"\"\n",
    "    results = graph.run(query)\n",
    "    subgraph_list = [(record[\"n.value\"], record[\"node_id\"]) for record in results]\n",
    "    subgraph_list_with_number = [f\"{i}. {subgraph[0]}\" for i, subgraph in enumerate(subgraph_list)]\n",
    "\n",
    "    sys_prompt_template = \"당신은 AWS 매뉴얼 문서에 정통한 전문 엔지니어입니다.\\n사용자의 질문에 가장 적절한 매뉴얼 문서 이름을 선택하는 것이 당신의 임무입니다. 관련된 문서가 없으면 빈 목록(\"\")을 제공합니다.\"\n",
    "    usr_prompt_template = \"주어진 질문에 가장 관련성 높은 문서 이름 1개를 선택해주세요.\\n#질문: {question}\\n\\n #문서 목록:\\n {subgraph_list_with_number}\\n\\n #응답 형식: 선택된 문서의 인덱스 번호만 제공 (서두는 생략)\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, subgraph_list_with_number=subgraph_list_with_number)\n",
    "    \n",
    "    selected_id = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    try:\n",
    "        if selected_id == \"\":\n",
    "            return GraphState(next_step=\"generate_answer\")\n",
    "\n",
    "        else: \n",
    "            selected_subgraph_id = subgraph_list[int(selected_id)][1]\n",
    "    except:\n",
    "        return GraphState(next_step=\"generate_answer\")\n",
    "    return GraphState(target_node=[selected_subgraph_id], next_step=\"traverse_child\", )\n",
    "\n",
    "def traverse_child(state: GraphState) -> GraphState: \n",
    "    question = state[\"question\"]\n",
    "    node_id = state[\"target_node\"][0]\n",
    "    child_list = []\n",
    "\n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title)-[:HAS_CHILD]->(c)\n",
    "        WHERE id(n) = $node_id\n",
    "        RETURN c.value, id(c) as child_id\n",
    "    \"\"\"\n",
    "    params = {\"node_id\": node_id}\n",
    "    results = graph.run(query, params)\n",
    "    child_list = [(record[\"c.value\"], record[\"child_id\"]) for record in results]\n",
    "    if not child_list:\n",
    "        return GraphState(next_step=\"get_contents\")\n",
    "    \n",
    "    child_list_with_number = [f\"{i}. {child}\" for i, child in enumerate(child_list)]\n",
    "    sys_prompt_template = \"당신은 AWS 매뉴얼 문서에 정통한 전문 엔지니어입니다.\\n사용자의 질문에 답변하기 위해 가장 적합한 하위 메뉴를 선택하는 것이 당신의 임무입니다.\"\n",
    "    usr_prompt_template = \"질문과 가장 관련성 높은 하위 메뉴를 3개 이하로 선택해주세요.\\n선택 기준:\\n1. 질문과 직접적으로 연관된 메뉴만 선택\\n2. 가장 관련성 높은 순서대로 정렬\\n3. 불필요하거나 관련 없는 메뉴는 제외\\n#질문: {question}\\n\\n #메뉴 목록:\\n {child_list_with_number}\\n\\n #응답 형식(선택된 메뉴의 인덱스 번호만 쉼표로 구분하여 나열): {csv_list_response_format}\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, child_list_with_number=child_list_with_number, csv_list_response_format=csv_list_response_format)\n",
    "    selected_ids = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    try:\n",
    "        if selected_ids == '\"\"' or selected_ids.strip() == \"\":\n",
    "            return GraphState(next_step=\"get_contents\")\n",
    "        else:\n",
    "            selected_id_list = [int(id.strip()) for id in selected_ids.split(',') if id.strip().isdigit()]\n",
    "            selected_childs = [child_list[id][1] for id in selected_id_list] if selected_id_list else []\n",
    "            return GraphState(next_step=\"traverse_child\", target_node=selected_childs)\n",
    "    except:\n",
    "        return GraphState(next_step=\"get_contents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. target_node: [0]\n",
      "Node value: Amazon Bedrock\n",
      "2. target_node: [1493, 508, 646]\n",
      "Node value: Agents for Amazon Bedrock\n",
      "3. target_node: [1718, 1509, 1497]\n",
      "Node value: Customize an Amazon Bedrock agent\n",
      "4. target_node: [1859, 1720, 1837]\n",
      "Node value: Optimize performance for Amazon Bedrock agents\n",
      "5. target_node: [1861]\n",
      "Node value: Optimize performance for Amazon Bedrock agents using a single knowledge base\n",
      "6. target_node: []\n",
      "Number of documents: 7\n",
      "Agents for Amazon Bedrock의 성능을 최적화하기 위해 다음과 같은 방법들을 사용할 수 있습니다:\n",
      "\n",
      "1. 단일 지식 베이스 사용: \n",
      "   - 에이전트에 하나의 지식 베이스만 포함되어 있는지 확인합니다.\n",
      "\n",
      "2. 액션 그룹 비활성화:\n",
      "   - 모든 액션 그룹을 비활성화하거나 제거합니다.\n",
      "\n",
      "3. 사용자 입력 요청 최소화:\n",
      "   - 에이전트가 추가 정보를 사용자에게 요청하지 않도록 설정합니다.\n",
      "\n",
      "4. 기본 오케스트레이션 프롬프트 템플릿 사용:\n",
      "   - 오케스트레이션 프롬프트 템플릿을 기본값으로 설정합니다.\n",
      "\n",
      "5. 콘솔에서 설정 확인:\n",
      "   - User input 필드가 비활성화되어 있는지 확인\n",
      "   - Knowledge bases 섹션에 하나의 지식 베이스만 있는지 확인\n",
      "   - Action groups 섹션에 액션 그룹이 없는지 확인\n",
      "   - Advanced prompts의 Orchestration 필드가 Default로 설정되어 있는지 확인\n",
      "\n",
      "6. API를 통한 확인:\n",
      "   - ListAgentKnowledgeBases 요청으로 단일 지식 베이스 확인\n",
      "   - ListAgentActionGroups 요청으로 액션 그룹 없음 확인\n",
      "   - GetAgent 요청으로 오케스트레이션 프롬프트 설정 확인\n",
      "\n",
      "7. 변경 사항 적용 및 테스트:\n",
      "   - 변경 사항을 적용한 후 테스트 창에서 에이전트의 성능을 테스트합니다.\n",
      "\n",
      "8. 필요시 새 버전 생성:\n",
      "   - 최적화된 설정으로 새로운 에이전트 버전을 배포합니다.\n",
      "\n",
      "이러한 방법들을 통해 Bedrock Agent의 성능을 최적화하고 단순한 사용 사례에서 지연 시간을 줄일 수 있습니다."
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def select_subgraph_dev(question, graph):\n",
    "    question = question\n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title {level: \"1\"})\n",
    "        RETURN n.value, id(n) as node_id\n",
    "    \"\"\"\n",
    "    results = graph.run(query)\n",
    "    subgraph_list = [(record[\"n.value\"], record[\"node_id\"]) for record in results]\n",
    "    subgraph_list_with_number = [f\"{i}. {subgraph[0]}\" for i, subgraph in enumerate(subgraph_list)]\n",
    "\n",
    "    sys_prompt_template = \"당신은 AWS 매뉴얼 문서에 정통한 전문 엔지니어입니다.\\n사용자의 질문에 가장 적절한 매뉴얼 문서 이름을 선택하는 것이 당신의 임무입니다. 관련된 문서가 없으면 빈 목록(\"\")을 제공합니다.\"\n",
    "    usr_prompt_template = \"주어진 질문에 가장 관련성 높은 문서 이름 1개를 선택해주세요.\\n#질문: {question}\\n\\n #문서 목록:\\n {subgraph_list_with_number}\\n\\n #응답 형식: 선택된 문서의 인덱스 번호만 제공 (서두는 생략)\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, subgraph_list_with_number=subgraph_list_with_number)\n",
    "    \n",
    "    selected_id = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    try:\n",
    "        if selected_id == \"\":\n",
    "            return [], \"generate_answer\"\n",
    "\n",
    "        else: \n",
    "            selected_subgraph_id = subgraph_list[int(selected_id)][1]\n",
    "            return [selected_subgraph_id], \"traverse_child\"\n",
    "    except:\n",
    "        return [], \"generate_answer\"\n",
    "\n",
    "def traverse_child_dev(question, graph, target_node):\n",
    "    question = question\n",
    "    node_id = target_node[0]\n",
    "    child_list = []\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE id(n) = $node_id\n",
    "        OPTIONAL MATCH (n)-[:HAS_CHILD]->(c)\n",
    "        RETURN n.value as node_value, c.value, id(c) as child_id\n",
    "    \"\"\"\n",
    "    params = {\"node_id\": node_id}\n",
    "    results = graph.run(query, params)\n",
    "\n",
    "    node_value = None\n",
    "    child_list = []\n",
    "\n",
    "    for record in results:\n",
    "        if node_value is None:\n",
    "            node_value = record[\"node_value\"]\n",
    "        if record[\"c.value\"] is not None:\n",
    "            child_list.append((record[\"c.value\"], record[\"child_id\"]))\n",
    "    \n",
    "    print(f\"Node value: {node_value}\")\n",
    "\n",
    "    if not child_list:\n",
    "        return node_id, [], \"get_contents\"\n",
    "\n",
    "    child_list_with_number = [f\"{i}. {child}\" for i, child in enumerate(child_list)]\n",
    "    sys_prompt_template = \"당신은 AWS 매뉴얼 문서에 정통한 전문 엔지니어입니다.\\n당신의 임무는 사용자의 질문에 답변하기 위해, 매뉴얼 문서에서 참고할 메뉴 항목을 선택하는 것입니다.\"\n",
    "    usr_prompt_template = \"\"\"\n",
    "    질문과 연관된 하위 메뉴의 인덱스 번호를 최대 3개 선택한 후, 연관성이 높은 메뉴부터 내림차순으로 정렬하여 답변합니다. \n",
    "    질문에 포함된 키워드와 정확히 일치하는 메뉴 항목에 가장 높은 우선순위를 부여하세요. \n",
    "    더 구체적이고 특정한 주제를 다루는 메뉴 항목에 높은 우선순위를 부여하세요. 예를 들어, 일반적인 'Getting started' 가이드보다는 특정 기능이나 서비스에 대한 상세 설명이 있는 항목을 선호합니다. \n",
    "    질문의 전체적인 맥락을 고려하여 가장 적절한 메뉴 항목을 선택하세요. 단순히 키워드가 일치한다고 해서 반드시 가장 연관성이 높은 것은 아닙니다.\n",
    "    \n",
    "    #질문: {question}\n",
    "     \n",
    "    #메뉴 목록:\n",
    "    {child_list_with_number}\n",
    "     \n",
    "    #응답 형식: {csv_list_response_format}. \n",
    "    (연관된 메뉴가 없으면 빈 목록(\"\")으로 응답하세요)\n",
    "    \"\"\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, child_list_with_number=child_list_with_number, csv_list_response_format=csv_list_response_format)\n",
    "    selected_ids = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    try:\n",
    "        if selected_ids == '\"\"' or selected_ids.strip() == \"\":\n",
    "            return node_id, [], \"get_contents\"\n",
    "        else:\n",
    "            selected_id_list = [int(id.strip()) for id in selected_ids.split(',') if id.strip().isdigit()]\n",
    "            selected_childs = [child_list[id][1] for id in selected_id_list] if selected_id_list else []\n",
    "            return node_id, selected_childs, \"traverse_child\"\n",
    "    except:\n",
    "        return node_id, [], \"get_contents\"\n",
    "\n",
    "def get_contents_dev(question, cur_pos):\n",
    "    question = question\n",
    "    node_id = cur_pos\n",
    "\n",
    "    count_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $node_id\n",
    "        RETURN count(c) as document_count\n",
    "    \"\"\"\n",
    "    params = {\"node_id\": node_id}\n",
    "    count_result = graph.run(count_query, params).data()[0]\n",
    "    document_count = count_result['document_count']\n",
    "\n",
    "    print(f\"Number of documents: {document_count}\")\n",
    "\n",
    "    if document_count <= 10:\n",
    "        content_query = \"\"\"\n",
    "            MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "            WHERE id(n) = $node_id\n",
    "            RETURN c.order, c.text\n",
    "            ORDER BY c.order\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "\n",
    "    #else:\n",
    "        #Vector search\n",
    "\n",
    "    content_results = graph.run(content_query, params)\n",
    "    contents = [record[\"c.text\"] for record in content_results]\n",
    "\n",
    "    return contents\n",
    "\n",
    "def get_sibling_contents_dev(cur_pos):\n",
    "    \n",
    "    return\n",
    "\n",
    "def generate_answer_dev(question, contents):\n",
    "    # Prompt setting\n",
    "    sys_prompt_template = \"당신은 AWS에 정통한 전문 엔지니어입니다. 주어진 사전 정보만 활용하여, 사용자 질문에 답변을 생성하세요. 사전 정보로 주어지지 않은 내용에 대한 질문에는 모른다고 답변하세요.\"\n",
    "    usr_prompt_template = \"#사전 정보: {context}\\n\\n #사용자 질문:\\n {question}\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", sys_prompt_template), (\"human\",usr_prompt_template)])\n",
    "\n",
    "    # Model setting\n",
    "    model_kwargs = {\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    llm = ChatBedrock(model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", region_name=\"us-west-2\", model_kwargs=model_kwargs, streaming=True)   \n",
    "\n",
    "    # Output setting\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    context = \" \".join(contents)\n",
    "    for chunk in chain.stream({\"context\": context, \"question\": question}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "question = \"Bedrock Agent 성능 최적화 방법\"\n",
    "target_node, next_step = select_subgraph_dev(question, graph)\n",
    "print(\"1. target_node:\", target_node)\n",
    "cur_pos, target_node, next_step = traverse_child_dev(question, graph, target_node)\n",
    "print(\"2. target_node:\", target_node)\n",
    "cur_pos, target_node, next_step = traverse_child_dev(question, graph, target_node)\n",
    "print(\"3. target_node:\", target_node)\n",
    "cur_pos, target_node, next_step = traverse_child_dev(question, graph, target_node)\n",
    "print(\"4. target_node:\", target_node)\n",
    "cur_pos, target_node, next_step = traverse_child_dev(question, graph, target_node)\n",
    "print(\"5. target_node:\", target_node)\n",
    "cur_pos, target_node, next_step = traverse_child_dev(question, graph, target_node)\n",
    "print(\"6. target_node:\", target_node)\n",
    "contents = get_contents_dev(question, cur_pos)\n",
    "\n",
    "generate_answer_dev(question, contents)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

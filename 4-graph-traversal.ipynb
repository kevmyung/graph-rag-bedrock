{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "#high_level_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "#high_level_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "high_level_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "low_level_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "def converse_with_bedrock(sys_prompt, usr_prompt, model_id):\n",
    "    temperature = 0\n",
    "    top_p = 0.1\n",
    "    top_k = 1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "\n",
    "def create_prompt(sys_template, user_template, **kwargs):\n",
    "    sys_prompt = [{\"text\": sys_template.format(**kwargs)}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template.format(**kwargs)}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "boto3_client = init_boto3_client(region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgraph_dev(question, graph):\n",
    "    question = question\n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title {level: \"1\"})\n",
    "        RETURN n.value, id(n) as node_id\n",
    "    \"\"\"\n",
    "    results = graph.run(query)\n",
    "    subgraph_list = [(record[\"n.value\"], record[\"node_id\"]) for record in results]\n",
    "    subgraph_list_with_number = [f\"{i}. {subgraph[0]}\" for i, subgraph in enumerate(subgraph_list)]\n",
    "\n",
    "    sys_prompt_template = \"\"\" \n",
    "    You are an expert engineer well-versed in AWS manual documents. \n",
    "    Your task is to select the most appropriate manual document name for the user's question. \n",
    "    If there are no relevant documents, provide an empty list (\"\"). \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\" \n",
    "    Please select the single most relevant document name for the given question.\n",
    "\n",
    "    #Question: {question}\n",
    "\n",
    "    #Document List: {subgraph_list_with_number}\n",
    "\n",
    "    #Response Format: Provide only the index number of the selected document (omit any preamble) \"\"\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, subgraph_list_with_number=subgraph_list_with_number)\n",
    "    \n",
    "    model_id = low_level_model \n",
    "    selected_id = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    try:\n",
    "        if selected_id == \"\":\n",
    "            return [], \"\", \"generate_answer\"\n",
    "\n",
    "        else: \n",
    "            selected_subgraph_id = subgraph_list[int(selected_id)][1]\n",
    "            print(\"Selected:\", subgraph_list[int(selected_id)][0])\n",
    "            return [selected_subgraph_id], subgraph_list[int(selected_id)][0], \"traverse_child\"\n",
    "    except:\n",
    "        return [], \"\", \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 제공하는 모델 목록\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] |  | generate_answer\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (현재 없는 내용)\n",
    "question = \"SageMaker에서 모델을 배포하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraverseResult:\n",
    "    def __init__(self, parent_id, parent_name, child_level, selected_child_ids, child_names, next_action):\n",
    "        self.parent_id = parent_id\n",
    "        self.parent_name = parent_name\n",
    "        self.child_level = child_level\n",
    "        self.selected_child_ids = selected_child_ids\n",
    "        self.child_names = child_names\n",
    "        self.next_action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def traverse_child_dev(question, subgraph, graph, target_node):\n",
    "    parent_id = target_node[0]\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE id(n) = $parent_id\n",
    "        OPTIONAL MATCH (n)-[:HAS_CHILD]->(c)\n",
    "        RETURN n.value as parent_name, c.level as child_level, c.value as child_name, id(c) as child_id\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    query_results = graph.run(query, params)\n",
    "\n",
    "    parent_name = None\n",
    "    child_level = None\n",
    "    child_list = []\n",
    "    child_names = []\n",
    "\n",
    "    for record in query_results:\n",
    "        if parent_name is None:\n",
    "            parent_name = record[\"parent_name\"]\n",
    "        if child_level is None:\n",
    "            child_level = record[\"child_level\"]\n",
    "        if record[\"child_name\"] is not None:\n",
    "            child_list.append((record[\"child_name\"], record[\"child_id\"]))\n",
    "            child_names.append(record[\"child_name\"])\n",
    "\n",
    "    print(f\"Traversing '{parent_name}'...\")\n",
    "\n",
    "    if not child_list:\n",
    "        print(\"No child. Proceed to 'get_contents'...\")\n",
    "        #print(f\"Debug: {parent_id}, {parent_name}, {child_level}, [], [], 'get_contents'\")\n",
    "        return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")\n",
    "\n",
    "    child_list_with_number = [f\"{i}. {child}\" for i, child in enumerate(child_list)]\n",
    "    sys_prompt_template = \"\"\"\n",
    "    당신은 AWS 매뉴얼 문서에 정통한 전문 엔지니어입니다.\n",
    "    당신의 임무는 사용자의 질문에 답변하기 위해, <{subgraph}> 매뉴얼 문서에서 가장 관련성 높은 하위 메뉴를 선택하는 것입니다.\n",
    "\n",
    "    작업 순서:\n",
    "    1. 주어진 하위 메뉴 목록을 검토하여 직접적으로 연관된 메뉴들을 찾습니다.\n",
    "    2. 연관성이 가장 높은 메뉴를 선택하여, 인덱스 번호(0부터 시작)로 응답합니다.\n",
    "    3. 질문과 매우 밀접한 메뉴가 1개 이상인 경우, 선택한 메뉴의 인덱스 번호 목록으로 응답합니다.\n",
    "\n",
    "    선택 기준:\n",
    "    - 질문의 핵심 키워드와 일치하고 질문의 맥락에 맞는 메뉴를 우선적으로 고려하세요.\n",
    "    - 일반적 가이드보다는 질문의 특정 주제나 기능을 다루는 메뉴를 선호합니다. 예를 들어, 'Getting started' 가이드보다는 특정 기능이나 서비스에 대한 상세 설명이 있는 항목을 선호합니다.\n",
    "    - 반드시 선택을 해야하는 것은 아닙니다. 연관성이 낮거나 불확실한 메뉴는 선택하지 마세요.\n",
    "\n",
    "    \"\"\"\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #질문: {question}\n",
    "\n",
    "    #메뉴 목록:\n",
    "    {child_list_with_number}\n",
    "\n",
    "    #응답 형식: {csv_list_response_format}.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, subgraph=subgraph, question=question, child_list_with_number=child_list_with_number, csv_list_response_format=csv_list_response_format)\n",
    "    model_id = high_level_model\n",
    "    selected_ids = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "    try:\n",
    "        selected_id_list = [int(id.strip()) for id in selected_ids.split(',') if id.strip().isdigit()]\n",
    "\n",
    "        if not selected_id_list:\n",
    "            #print(f\"Debug1: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "            return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")\n",
    "        \n",
    "        selected_child_ids = [child_list[id][1] for id in selected_id_list if id < len(child_list)]\n",
    "        selected_child_names = [child_list[id][0] for id in selected_id_list if id < len(child_list)]\n",
    "\n",
    "        #print(f\"Debug2: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "        return TraverseResult(parent_id, parent_name, child_level, selected_child_ids, selected_child_names, \"traverse_child\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        #print(f\"Debug3: Exception occurred: {str(e)}\")\n",
    "        #print(f\"Debug4: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "        return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, parent_id, parent_name, contents, contents_length, search_type, k):\n",
    "        self.parent_id = parent_id\n",
    "        self.parent_name = parent_name\n",
    "        self.contents = contents\n",
    "        self.contents_length = contents_length\n",
    "        self.search_type = search_type\n",
    "        self.k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1908]\n",
      "Traversing 'Custom models'...\n",
      "target_node: [2023]\n",
      "Traversing 'Use a custom model'...\n",
      "No child. Proceed to 'get_contents'...\n"
     ]
    }
   ],
   "source": [
    "traverse_results = []\n",
    "\n",
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Custom Model 활용 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_dev(graph, parent_id, k=5):\n",
    "    count_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id\n",
    "        RETURN count(c) as contents_length, n.value as parent_name\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    count_result = graph.run(count_query, params).data()[0]\n",
    "    contents_length = count_result['contents_length']\n",
    "    parent_name = count_result['parent_name']\n",
    "    print(f\"Num Documents: {contents_length}\")\n",
    "\n",
    "    if contents_length <= k * 2:\n",
    "        search_type = \"get_short_documents\"\n",
    "        content_query = \"\"\"\n",
    "            MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "            WHERE id(n) = $parent_id\n",
    "            RETURN c.text\n",
    "            ORDER BY c.order\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "        params = {\"parent_id\": parent_id, \"k\": k}\n",
    "        content_results = graph.run(content_query, params)\n",
    "        contents = [record[\"c.text\"] for record in content_results]\n",
    "        context = \" \".join(contents)\n",
    "\n",
    "    else:\n",
    "        search_type = \"node_level_search\"\n",
    "        context = \"\"\n",
    "\n",
    "    return Context(parent_id, parent_name, context, contents_length, search_type, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [116]\n",
      "Traversing 'Supported foundation models in Amazon Bedrock'...\n",
      "target_node: [134]\n",
      "Traversing 'Model support by feature'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 6\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "Note You can run inference on all available FMs. The following table details the support for features that are limited to certain FMs. Model support by feature 45 |Model|Model evaluati n|Knowled o base (embed gs)|gKen owled base din(q uery)|gAeg ents|Fine- tuning (custom models)|Continu pre- train ing (custom models)|edP rovisio ed Through t|n Tool use pu|Convers API| |---|---|---|---|---|---|---|---|---|---| |AI21 Jamba- Ins truct|No|N/A|No|No|No|No|No|No|Yes| |Amazon Titan Text G1 - Express|Yes|N/A|No|No|Yes|Yes|Yes|No|Yes| |Amazon Titan Text G1 - Lite|Yes|N/A|No|No|Yes|Yes|Yes|No|Yes| |Amazon Titan Text Premier|No|N/A|Yes|Yes|Yes (preview|No )|Yes (preview|No )|Yes| |Amazon Titan Embedd s G1 - Text|No ing|N/A|No|No|No|No|Yes|No|No| |Amazon Titan Multimo|No da|Yes|No|No|Yes|No|Yes|No|No| Model support by feature 46 |Col1|Col2|gs)|Col4|Col5|models)|ing (custom models)|t|Col9|Col10| |---|---|---|---|---|---|---|---|---|---| |l Embedd s G1|ing||||||||| |Amazon Titan Image Generat G1 V1|No or|N/A|No|No|Yes|No|Yes|No|No| |Amazon Titan Image Generat G1 V2|No or|N/A|No|No|Yes|No|Yes|No|No| |Anthrop Claude v1|ic Yes|N/A|No|No|No|No|Yes|No|Yes| |Anthrop Claude v2|ic Yes|N/A|Yes|Yes|No|No|Yes|No|Yes| |Anthrop Claude v2.1|ic Yes|N/A|Yes|Yes|No|No|Yes|No|Yes| |Anthrop Claude Instant|ic Yes|N/A|Yes|Yes|No|No|Yes|No|Yes| Model Model Knowledge Knowledge Agents Fine- Continued Provision Tool Converse evaluatio base base tuning pre- ed use API n (embeddin (query) (custom train Throughpu gs) models) ing t (custom models) Model support by feature 47 |Col1|Col2|gs)|Col4|Col5|models)|ing (custom models)|t|Col9|Col10| |---|---|---|---|---|---|---|---|---|---| |Anthrop Claude 3 Sonnet|ic Yes|N/A|Yes|Yes|No|No|Yes|Yes|Yes| |Anthrop Claude 3.5 Sonnet|ic Yes|N/A|No|No|No|No|No|Yes|Yes| |Anthrop Claude 3 Haiku|ic Yes|N/A|Yes|Yes|Yes (preview|No )|Yes|Yes|Yes| |Anthrop Claude 3 Opus|ic Yes|N/A|No|Yes|No|No|No|Yes|Yes| |AI21 Labs Jurassic- 2 Mid|Yes|No|No|No|No|No|No|No|Yes (Limited| |AI21 Labs Jurassic- 2 Ultra|Yes|No|No|No|No|No|Yes|No|Yes (Limited| Model Model Knowledge Knowledge Agents Fine- Continued Provision Tool Converse evaluatio base base tuning pre- ed use API n (embeddin (query) (custom train Throughpu gs) models) ing t (custom models) Model support by feature 48 |Col1|Col2|gs)|Col4|Col5|models)|ing (custom models)|t|Col9|Col10| |---|---|---|---|---|---|---|---|---|---| |Cohere Comman|Yes d|N/A|No|No|Yes|No|Yes|No|Yes (Limited| |Cohere Comman Light|Yes d|N/A|No|No|Yes|No|Yes|No|Yes (Limited| |Cohere Comman R|No d|No|No|No|No|No|No|Yes|Yes| |Cohere Comman R+|No d|No|No|No|No|No|No|Yes|Yes| |Cohere Embed English|No|Yes|No|No|No|No|Yes|No|No| |Cohere Embed Multiling ual|No|Yes|No|No|No|No|Yes|No|No| |Meta Llama 2 Chat 13B|Yes|N/A|No|No|No|No|Yes|No|Yes| Model Model Knowledge Knowledge Agents Fine- Continued Provision Tool Converse evaluatio base base tuning pre- ed use API n (embeddin (query) (custom train Throughpu gs) models) ing t (custom models) Model support by feature 49 |Col1|Col2|gs)|Col4|Col5|models)|ing (custom models)|t|Col9|Col10| |---|---|---|---|---|---|---|---|---|---| |Meta Llama 2 Chat 70B|Yes|N/A|No|No|No|No|No|No|Yes| |Meta Llama 2 13B|No|N/A|No|No|Yes|No|Yes (see note below)|No|Yes| |Meta Llama 2 70B|No|N/A|No|No|Yes|No|Yes (see note below)|No|Yes| |Meta Llama 2 70B|No|N/A|No|No|Yes|No|Yes (see note below)|No|Yes| |Meta Llama 3 8B Instruct|Yes|N/A|No|No|No|No|No|No|Yes| |Meta Llama 3 70B Instruct|Yes|N/A|No|No|No|No|No|No|Yes| Model Model Knowledge Knowledge Agents Fine- Continued Provision Tool Converse evaluatio base base tuning pre- ed use API n (embeddin (query) (custom train Throughpu gs) models) ing t (custom models) Model support by feature 50 |Col1|Col2|gs)|Col4|Col5|models)|ing (custom models)|t|Col9|Col10| |---|---|---|---|---|---|---|---|---|---| |Meta Llama 3.1 8B Instruct|Yes|N/A|No|No|No|No|No|Yes|Yes| |Meta Llama 3.1 70B Instruct|Yes|N/A|No|No|No|No|No|Yes|Yes| |Meta Llama 3.1 405B Instruct|Yes|N/A|No|No|Yes|No|No|Yes|Yes| |Mistral AI Mistral 7B Instruct|Yes|N/A|No|No|No|No|No|No|Yes| |Mistral AI Mistral Large|Yes|N/A|No|No|No|No|No|Yes|Yes| Model Model Knowledge Knowledge Agents Fine- Continued Provision Tool Converse evaluatio base base tuning pre- ed use API n (embeddin (query) (custom train Throughpu gs) models) ing t (custom models) Model support by feature 51 |Col1|Col2|gs)|Col4|Col5|models)|ing (custom models)|t|Col9|Col10| |---|---|---|---|---|---|---|---|---|---| |Mistral AI Mistral Large 2 (24.07)|No|No|No|No|No|No|No|No|Yes| |Mistral AI Mixtral 8X7B Instruct|Yes|N/A|No|No|No|No|No|No|Yes| |Mistral AI Mistral Small|Yes|N/A|No|No|No|No|No|Yes|Yes| |Stable Diffusion XL 0.8|No|N/A|No|No|No|No|No|No|No| |Stable Diffusion XL 1.x|No|N/A|No|No|No|No|Yes|No|No| Model Model Knowledge Knowledge Agents Fine- Continued Provision Tool Converse evaluatio base base tuning pre- ed use API n (embeddin (query) (custom train Throughpu gs) models) ing t (custom models)\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 활용가능한 모델 목록\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1359]\n",
      "Traversing 'Agents for Amazon Bedrock'...\n",
      "target_node: [1578]\n",
      "Traversing 'Customize an Amazon Bedrock agent'...\n",
      "target_node: [1581]\n",
      "Traversing 'Advanced prompts in Amazon Bedrock'...\n",
      "target_node: [1606]\n",
      "Traversing 'Parser Lambda function in Agents for Amazon Bedrock'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 82\n",
      "==============\n",
      "Search Type: node_level_search\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock의 Agent에서 SDK를 활용해서 Agent Action을 정의하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"full_text\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def node_level_search_dev(question, graph, parent_id, parent_name, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "            당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 벡터 검색으로 문서를 찾아내기에 적합한 질문을 만들어주세요.({language})\n",
    "            \n",
    "            주의: \n",
    "\n",
    "            - 문서 이름과 질문을 고려하여 해당 문서 내에서 가장 관련성 높고 특징적인 질문을 생성하세요.\n",
    "            - 문서의 특정 내용을 잘 나타내는 자연어 질문을 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#검색 대상 문서 이름:\\n{subgraph}\\n\\n#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_level_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "        index_name = \"content_embedding_index\"\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=region_name)\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "            MATCH (parent)-[:HAS_CONTENTS]->(child:Content)\n",
    "            WHERE id(parent) = $parent_id\n",
    "            WITH child\n",
    "            CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "            WHERE node = child\n",
    "            RETURN id(node) AS node_id, node.text AS text, score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "        \n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 핵심 키워드를 1개 추출합니다.\n",
    "        키워드는 다음 조건을 반드시 만족해야 합니다:\n",
    "        1. 키워드에 '_', '-' 등의 특수 문자 포함 금지 (예: custom_model 대신 custom model로 응답)\n",
    "        2. 주어진 문서 이름 내에서 질문의 맥락에 가장 적합한 단어를 선택 (문서 이름은 키워드에 포함할 필요가 없음)\n",
    "        2. 문서 이름에 이미 포함된 내용보다는 검색하려는 특정 기능 및 개념을 잘 나타내는 구체적 단어를 선택\n",
    "        3. {language} 키워드 제공 \n",
    "        \n",
    "        주의: \n",
    "        - 문서 이름과 질문을 고려하여 해당 문서 내에서 가장 관련성 높고 특징적인 단어를 선택하세요.\n",
    "        - 너무 일반적인 단어보다는 문서의 특정 내용을 잘 나타내는 단어를 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#검색 대상 문서 이름:\\n{parent_name}\\n\\n#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, parent_name=parent_name, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_level_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "\n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = child\n",
    "                RETURN node.text as text, score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child, $keywords AS keyword\n",
    "                WHERE child.text CONTAINS keyword\n",
    "                RETURN child.text AS text, \n",
    "                    size(split(toLower(child.text), toLower(keyword))) - 1 AS score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"parent_id\": parent_id, \"keywords\": keywords, \"k\": k}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1359]\n",
      "Traversing 'Agents for Amazon Bedrock'...\n",
      "target_node: [1393]\n",
      "Traversing 'Create an action group for an Amazon Bedrock agent'...\n",
      "target_node: [1396]\n",
      "Traversing 'Defining actions in the action group'...\n",
      "target_node: [1404]\n",
      "Traversing 'Define OpenAPI schemas for your agent's action groups in Amazon Bedrock'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 13\n",
      "==============\n",
      "Search Type: node_level_search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock의 Agent에서 SDK를 활용해서 Agent Action을 정의하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    search_content = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(search_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevance_dev(question, content, parent_name, contents_length, search_type, k=5):\n",
    "    optional_prompt1 = \"\"\n",
    "    optional_prompt2 = \"\"\n",
    "    \n",
    "    if search_type == \"get_short_documents\" and contents_length > k:\n",
    "        optional_prompt1 = \"- 사전 정보가 질문 취지에 부합하지만 뒷 내용 추가 확인 필요: 'Partial'\"\n",
    "        optional_prompt2 = \"또는 `Partial`\"\n",
    "\n",
    "    sys_prompt_template = \"\"\"\n",
    "    당신은 유능한 데이터 분석가입니다. 당신의 임무는 오직 주어진 사전 정보만을 활용하여 질문에 답변 가능한지, 아래의 기준으로 판단하는 것입니다.\n",
    "    \n",
    "    판단 기준:\n",
    "    1. 질문의 핵심 키워드가 문서 이름 또는 사전 정보에 등장하는가?\n",
    "    2. 알아내고자 하는 구체적 정보가 포함되어 있는가?\n",
    "\n",
    "    응답 방법:\n",
    "    - 문서 이름 및 사전 정보가 질문과 관련 없음: 'None'\n",
    "    - 사전 정보만으로 질문에 답변 가능: 'Complete'\n",
    "    {partial1}\n",
    "\n",
    "    서두는 생략하고, `None` 또는 `Complete`{partial2}으로만 답변하세요.\n",
    "    \"\"\"\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #사전 정보 (문서이름: {parent_name})\n",
    "    {context}\n",
    "    \n",
    "    #질문: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, partial1=optional_prompt1, partial2=optional_prompt2, parent_name=parent_name, question=question, context=content)\n",
    "    model_id = high_level_model\n",
    "    status = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1908]\n",
      "Traversing 'Custom models'...\n",
      "target_node: [1944]\n",
      "Traversing 'Submit a model customization job'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 6\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "You can create a custom model by using Fine-tuning or Continued Pre-training in the Amazon Bedrock console or API. The customization job can take several hours. The duration of the job depends on the size of the training data (number of records, input tokens, and output tokens), number of epochs, and batch size. Select the tab corresponding to your method of choice and follow the steps. Console To submit a model customization job in the console, carry out the following steps. 1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock [permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/](https://console.aws.amazon.com/bedrock/) [bedrock/.](https://console.aws.amazon.com/bedrock/) Submit a job 918 2. From the left navigation pane, choose Custom models under Foundation models. 3. In the Models tab, choose Customize model and then Create Fine-tuning job or Create Continued Pre-training job, depending on the type of model you want to train. 4. In the Model details section, do the following. a. Choose the model that you want to customize with your own data and give your resulting model a name. b. (Optional) By default, Amazon Bedrock encrypts your model with a key owned and managed by AWS. To use a custom KMS key, select Model encryption and choose a key. c. (Optional) To associate tags with the custom model, expand the Tags section and select Add new tag. 5. In the Job configuration section, enter a name for the job and optionally add any tags to associate with the job. 6. (Optional) To use a virtual private cloud (VPC) to protect your training data and customization job, select a VPC that contains the input data and output data Amazon S3 locations, its subnets, and security groups in the VPC settings section. Note If you include a VPC configuration, the console cannot create a new service role for the job. Create a custom service role and add permissions similar to the example described in Attach VPC permissions to a model customization role. 7. In the Input data section, select the S3 location of the training dataset file and, if applicable, the validation dataset file. 8. In the Hyperparameters section, input values for hyperparameters to use in training. 9. In the Output data section, enter the Amazon S3 location where Amazon Bedrock should save the output of the job. Amazon Bedrock stores the training loss metrics and validation loss metrics for each epoch in separate files in the location that you specify. 10. In the Service access section, select one of the following: - Use an existing service role – Select a service role from the drop-down list. For more information on setting up a custom role with the appropriate permissions, see Create a service role for model customization. - Create and use a new service role – Enter a name for the service role. Submit a job 919 11. Choose Fine-tune model or Create Continued Pre-training job to begin the job. API Request [Send a CreateModelCustomizationJob (see link for request and response formats and](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateModelCustomizationJob.html) [field details) request with an Amazon Bedrock control plane endpoint to submit a model](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#br-cp) customization job. Minimally, you must provide the following fields. - roleArn – The ARN of the service role with permissions to customize models. Amazon Bedrock can automatically create a role with the appropriate permissions if you use the console, or you can create a custom role by following the steps at Create a service role for model customization. Note If you include a vpcConfig field, make sure that the role has the proper permissions to access the VPC. For an example, see Attach VPC permissions to a model customization role. - baseModelIdentifier – The model ID or ARN of the foundation model to customize. - customModelName – The name to give the newly customized model. - jobName – The name to give the training job. - hyperParameters – Hyperparameters that affect the model customization process. - trainingDataConfig – An object containing the Amazon S3 URI of the training dataset. Depending on the customization method and model, you can also include a --- validationDataConfig. For more information about preparing the datasets, see Prepare --- the datasets. - outputDataConfig – An object containing the Amazon S3 URI to write the output data to. If you don't specify the customizationType, the model customization method defaults to --- FINE_TUNING. --- To prevent the request from completing more than once, include a clientRequestToken. You can include the following optional fields for extra configurations. Submit a job 920 - jobTags and/or customModelTags – Associate tags with the customization job or resulting custom model. - customModelKmsKeyId – Include a custom KMS key to encrypt your custom model. - vpcConfig – Include the\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"model customization 작업에서 데이터 암호화하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    search_content = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "    status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "    status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1908]\n",
      "Traversing 'Custom models'...\n",
      "target_node: [2023]\n",
      "Traversing 'Use a custom model'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 3\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "Before you can use a customized model, you need to purchase Provisioned Throughput for it. For more information about Provisioned Throughput, see Provisioned Throughput for Amazon Bedrock. You can then use the resulting provisioned model for inference. Select the tab corresponding to your method of choice and follow the steps. Console To purchase Provisioned Throughput for a custom model. 1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock [permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/](https://console.aws.amazon.com/bedrock/) [bedrock/.](https://console.aws.amazon.com/bedrock/) 2. From the left navigation pane, choose Custom models under Foundation models. Use a custom model 946 3. In the Models tab, choose the radio button next to the model for which you want to buy Provisioned Throughput or select the model name to navigate to the details page. 4. Select Purchase Provisioned Throughput. 5. For more details, follow the steps at Purchase a Provisioned Throughput for a Amazon Bedrock model. 6. After purchasing Provisioned Throughput for your custom model, follow the steps at Use a Provisioned Throughput. When you carry out any operation that supports usage of custom models, you will see your custom model as an option in the model selection menu. API To purchase Provisioned Throughput for a custom model, follow the steps at Purchase a Provisioned Throughput for a Amazon Bedrock model to send a [CreateProvisionedModelThroughput (see link for request and response formats and field](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateProvisionedModelThroughput.html) [details) request with a Amazon Bedrock control plane endpoint. Use the name or ARN of your](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#br-cp) custom model as the modelId. The response returns a provisionedModelArn that you [can use as the modelId when making an InvokeModel or InvokeModelWithResponseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) request. See code examples\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock의 Custom Model 활용 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    search_content = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "    status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "    status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1359]\n",
      "Traversing 'Agents for Amazon Bedrock'...\n",
      "target_node: [1454]\n",
      "Traversing 'Use memory to retain conversational context across multiple sessions'...\n",
      "target_node: [1458]\n",
      "Traversing 'Configure memory for your Amazon Bedrock agent'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 4\n",
      "context: Configure memory for your Amazon Bedrock agent\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "To configure memory for your agent, you must first enable memory and then optionally specify [the retention period for the memory. You can enable memory for your agent when you create or](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-create.html) [update your agent.](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-manage.html#agents-edit) To learn how to configure memory for your agent, select the tab corresponding to your method of choice and follow steps. Console To configure memory for your agent 1. If you're not already in the agent builder, do the following: a. Sign in to the AWS Management Console using an IAM role with Amazon [Bedrock permissions, and open the Amazon Bedrock console at https://](https://console.aws.amazon.com/bedrock/) [console.aws.amazon.com/bedrock/.](https://console.aws.amazon.com/bedrock/) b. Select Agents from the left navigation pane. Then, choose an agent in the Agents section. c. Choose Edit in Agent Builder 2. In the Agent details section, for Select model, make sure to select either Claude 3 Sonnet or Claude 3 Haiku. 3. In the Memory section, do the following: a. Select Enabled. b. (Optional) By default, agent retains conversational context for 30 days. To configure a custom retention period, enter a number between 1 and 30 to specify the memory duration for your agent. 4. Make sure to first Save and then Prepare to apply the changes you have made to the agent before testing it. API [To enable and configure memory for your agent, send an CreateAgent or UpdateAgent request](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_CreateAgent.html) [with an Agents for Amazon Bedrock build-time endpoint.](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#bra-bt) Configure memory for Amazon Bedrock agent 692 In the Amazon Bedrock API, you specify the memoryConfiguration when you send a an [CreateAgent or UpdateAgent request.](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_CreateAgent.html) The following shows the general format of the memoryConfiguration: \"memoryConfiguration\": {\n",
      "\"enabledMemoryTypes\": [ \"SESSION_SUMMARY\" ],\n",
      "\"storageDays\":30\n",
      "},  \n",
      "---  \n",
      "You can optionally configure the memory retention period by assigning the storageDays with\n",
      "a number between 1 and 30 days.  \n",
      "Note  \n",
      "If you enable memory for the agent and do not specify memoryId when you invoke the\n",
      "agent, agent will not store that specific turn in the memory.\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock Agent에서 memory 기능을 활용하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"context:\", context.parent_name)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    search_content = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "    status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "    status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling_contents_dev(graph, parent_id, content, k=5):\n",
    "    content_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id AND c.order >= $order_pos\n",
    "        RETURN c.text\n",
    "        ORDER BY c.order\n",
    "        LIMIT $k\n",
    "    \"\"\"       \n",
    "    trial = 1\n",
    "    order_pos = k * trial\n",
    "    params = {\"parent_id\": parent_id, \"k\": k, \"order_pos\": order_pos} \n",
    "\n",
    "    content_results = graph.run(content_query, params)\n",
    "    sibling_content = [record[\"c.text\"] for record in content_results]\n",
    "    contents = \" \".join([content] + sibling_content)\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1070]\n",
      "Traversing 'Knowledge bases for Amazon Bedrock'...\n",
      "target_node: [1175]\n",
      "Traversing 'Data source connectors'...\n",
      "target_node: [1258]\n",
      "Traversing 'Crawl web pages for your Amazon Bedrock knowledge base'...\n",
      "Num Documents: 2\n",
      "context: Crawl web pages for your Amazon Bedrock knowledge base\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "Note Crawling web URLs as your data source is in preview release and is subject to change. The Amazon Bedrock provided Web Crawler connects to and crawls URLs you have selected for use in your Amazon Bedrock knowledge base. You can crawl website pages in accordance with [your set scope or limits for your selected URLs. You can crawl website pages using either the AWS](https://console.aws.amazon.com/bedrock/home) [Management Console for Amazon Bedrock or the CreateDataSource API (see Amazon Bedrock](https://console.aws.amazon.com/bedrock/home) [supported SDKs and AWS CLI).](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) [When selecting websites to crawl, you must adhere to the Amazon Acceptable Use Policy and all](https://aws.amazon.com/aup/) other Amazon terms. Remember that you must only use the Web Crawler to index your own web pages, or web pages that you have authorization to crawl. [The Web Crawler respects robots.txt in accordance with the RFC 9309](https://www.rfc-editor.org/rfc/rfc9309.html) There are limits to how many web page content items and MB per content item that can be [crawled. See Quotas for knowledge bases.](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html) Topics - Supported features - Prerequisites - Connection configuration\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Knowledgebase의 소스로 웹 페이지를 사용하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [2672]\n",
      "Traversing 'Code examples for Amazon Bedrock using AWS SDKs'...\n",
      "target_node: [2728]\n",
      "Traversing 'Code examples for Amazon Bedrock Runtime using AWS SDKs'...\n",
      "target_node: [3004]\n",
      "Traversing 'Cohere Command for Amazon Bedrock Runtime using AWS SDKs'...\n",
      "target_node: [3018]\n",
      "Traversing 'Invoke Cohere Command on Amazon Bedrock using Bedrock's Converse API with a response stream'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 9\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "The following code examples show how to send a text message to Cohere Command, using Bedrock's Converse API and process the response stream in real-time. .NET AWS SDK for .NET Note There's more on GitHub. Find the complete example and learn how to set up and run [in the AWS Code Examples Repository.](https://github.com/awsdocs/aws-doc-sdk-examples/tree/main/dotnetv3/Bedrock-runtime#code-examples) Send a text message to Cohere Command, using Bedrock's Converse API and process the response stream in real-time. --- // Use the Converse API to send a text message to Cohere Command // and print the response stream. using System; using System.Collections.Generic; using System.Linq; using Amazon; using Amazon.BedrockRuntime; using Amazon.BedrockRuntime.Model; --- Cohere Command 1383 --- // Create a Bedrock Runtime client in the AWS Region you want to use. var client = new AmazonBedrockRuntimeClient(RegionEndpoint.USEast1); // Set the model ID, e.g., Command R. var modelId = \"cohere.command-r-v1:0\"; // Define the user message. var userMessage = \"Describe the purpose of a 'hello world' program in one line.\"; // Create a request with the model ID, the user message, and an inference configuration. var request = new ConverseStreamRequest { ModelId = modelId, Messages = new List<Message> { new Message { Role = ConversationRole.User, Content = new List<ContentBlock> { new ContentBlock { Text = userMessage } } } }, InferenceConfig = new InferenceConfiguration() { MaxTokens = 512, Temperature = 0.5F, TopP = 0.9F } }; try { // Send the request to the Bedrock Runtime and wait for the result. var response = await client.ConverseStreamAsync(request); // Extract and print the streamed response text in real-time. foreach (var chunk in response.Stream.AsEnumerable()) { if (chunk is ContentBlockDeltaEvent) { Console.Write((chunk as ContentBlockDeltaEvent).Delta.Text); } --- Cohere Command 1384 --- } } catch (AmazonBedrockRuntimeException e) { Console.WriteLine($\"ERROR: Can't invoke '{modelId}'. Reason: {e.Message}\"); throw; } --- [• For API details, see ConverseStream in AWS SDK for .NET API Reference.](https://docs.aws.amazon.com/goto/DotNetSDKV3/bedrock-runtime-2023-09-30/ConverseStream) Java SDK for Java 2.x Note There's more on GitHub. Find the complete example and learn how to set up and run [in the AWS Code Examples Repository.](https://github.com/awsdocs/aws-doc-sdk-examples/tree/main/javav2/example_code/bedrock-runtime#readme) Send a text message to Cohere Command, using Bedrock's Converse API and process the response stream in real-time. --- // Use the Converse API to send a text message to Cohere Command // and print the response stream. import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider; import software.amazon.awssdk.regions.Region; import software.amazon.awssdk.services.bedrockruntime.BedrockRuntimeAsyncClient; import software.amazon.awssdk.services.bedrockruntime.model.ContentBlock; import software.amazon.awssdk.services.bedrockruntime.model.ConversationRole; import software.amazon.awssdk.services.bedrockruntime.model.ConverseStreamResponseHandler; import software.amazon.awssdk.services.bedrockruntime.model.Message; import java.util.concurrent.ExecutionException; public class ConverseStream { public static void main(String[] args) { --- Cohere Command 1385 --- // Create a Bedrock Runtime client in the AWS Region you want to use. // Replace the DefaultCredentialsProvider with your preferred credentials provider. var client = BedrockRuntimeAsyncClient.builder() .credentialsProvider(DefaultCredentialsProvider.create()) .region(Region.US_EAST_1) .build(); // Set the model ID, e.g., Command R. var modelId = \"cohere.command-r-v1:0\"; // Create the input text and embed it in a message object with the user role. var inputText = \"Describe the purpose of a 'hello world' program in one line.\"; var message = Message.builder() .content(ContentBlock.fromText(inputText)) .role(ConversationRole.USER) .build(); // Create a handler to extract and print the response text in real-time. var responseStreamHandler = ConverseStreamResponseHandler.builder() .subscriber(ConverseStreamResponseHandler.Visitor.builder() .onContentBlockDelta(chunk -> { String responseText = chunk.delta().text(); System.out.print(responseText); }).build() ).onError(err -> System.err.printf(\"Can't invoke '%s': %s\", modelId, err.getMessage()) ).build(); try { // Send the message with a basic inference configuration and attach the handler. client.converseStream(request -> request.modelId(modelId) .messages(message) .inferenceConfig(config -> config .maxTokens(512) .temperature(0.5F) .topP(0.9F) ), responseStreamHandler).get(); --- Cohere Command 1386 --- } catch (ExecutionException | InterruptedException e) { System.err.printf(\"Can't invoke '%s': %s\", modelId, e.getCause().getMessage()); } } } --- [• For API details, see ConverseStream in AWS SDK for Java 2.x API\n",
      "==============\n",
      "Partial\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 \n",
    "question = \"Bedrock에서 Converse API를 활용할 때 응답 양식\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [3333]\n",
      "Traversing 'Quotas for Amazon Bedrock'...\n",
      "target_node: [3346]\n",
      "Traversing 'Model inference prompt quotas'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 2\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "Select a tab to see model-specific quotas for prompts. Amazon Titan Text models |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text prompt length, in characters|42,000|No| Amazon Titan Image Generator G1 V1 |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text prompt length, in characters|1,024|No| Model inference prompt quotas 1574 |Description|Value|Adjustable through Service Quotas| |---|---|---| |Input image size|5 MB|No| |Input image height in pixels (inpainting/outpainting)|1,024|No| |Input image width in pixels (inpainting/outpainting)|1,024|No| |Input image height in pixels (image variation)|4,096|No| |Input image width in pixels (image variation)|4,096|No| |Input image total pixels|12,582,912|No| Amazon Titan Embeddings G1 - Text |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text input length, in characters|50,000|No| Amazon Titan Multimodal Embeddings G1 |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text input length, in characters|100,000|No| |Base64-encoded string of image, in characters|25,000,000|No| Model inference prompt quotas 1575\n",
      "==============\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (찾을 수 없는 정보)\n",
    "question = \"Bedrock의 가격 정책\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"keyword\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def subgraph_level_search_dev(question, graph, subgraph, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        print(\"vector search started\")\n",
    "\n",
    "        sys_prompt_template = \"\"\"\n",
    "            당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 벡터 검색으로 문서를 찾아내기에 적합한 질문을 만들어주세요.({language})\n",
    "            \n",
    "            주의: \n",
    "\n",
    "            - 문서 이름과 질문을 고려하여 해당 문서 내에서 가장 관련성 높고 특징적인 질문을 생성하세요.\n",
    "            - 문서의 특정 내용을 잘 나타내는 자연어 질문을 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#검색 대상 문서 이름:\\n{subgraph}\\n\\n#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_level_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "        print(keywords)\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=region_name)\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "        MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "        MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "\n",
    "        CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "        WHERE node = content\n",
    "\n",
    "        RETURN node.text AS text, score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"subgraph\": subgraph,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        print(\"text search started\")\n",
    "        sys_prompt_template = \"\"\"\n",
    "        당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 핵심 키워드를 1개 추출합니다.\n",
    "        키워드는 다음 조건을 반드시 만족해야 합니다:\n",
    "        1. 키워드에 '_', '-' 등의 특수 문자 포함 금지 (예: custom_model 대신 custom model로 응답)\n",
    "        2. 주어진 문서 이름 내에서 질문의 맥락에 가장 적합한 단어를 선택 (문서 이름은 키워드에 포함할 필요가 없음)\n",
    "        2. 문서 이름에 이미 포함된 내용보다는 검색하려는 특정 기능 및 개념을 잘 나타내는 구체적 단어를 선택\n",
    "        3. {language} 키워드 제공 \n",
    "        \n",
    "        주의: \n",
    "        - 문서 이름과 질문을 고려하여 해당 문서 내에서 가장 관련성 높고 특징적인 단어를 선택하세요.\n",
    "        - 너무 일반적인 단어보다는 문서의 특정 내용을 잘 나타내는 단어를 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#검색 대상 문서 이름:\\n{subgraph}\\n\\n#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, subgraph=subgraph, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_level_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "    \n",
    "        \n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"\n",
    "                MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "                MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = content\n",
    "\n",
    "                RETURN node.text as text, score, title.name as title_name, title.level as title_level\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"\n",
    "                MATCH (root:Title {level: \"1\", value: $subgraph})\n",
    "                MATCH (root)-[:HAS_CHILD*0..]->(title:Title)-[:HAS_CONTENTS]->(content:Content)\n",
    "                WITH content, title, $keywords AS keyword\n",
    "                WHERE content.text CONTAINS keyword\n",
    "                RETURN content.text AS text, \n",
    "                    size(split(toLower(content.text), toLower(keyword))) - 1 AS score,\n",
    "                    {\n",
    "                        title: title.name,\n",
    "                        level: title.level,\n",
    "                        value: title.value\n",
    "                    } AS metadata\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = { \"subgraph\": subgraph, \"k\": k, \"keywords\": keywords}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [3333]\n",
      "Traversing 'Quotas for Amazon Bedrock'...\n",
      "Num Documents: 3\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "Your AWS account has default quotas, formerly referred to as limits, for Amazon Bedrock. To view [service quotas for Amazon Bedrock, follow the steps at Viewing service quotas and select Amazon](https://docs.aws.amazon.com/servicequotas/latest/userguide/gs-request-quota.html) Bedrock as the service. Some quotas differ by model. Unless specified otherwise, a quota applies to all versions of a model. To maintain the performance of the service and to ensure appropriate usage of Amazon Bedrock, the default quotas assigned to an account might be updated depending on regional factors, payment history, fraudulent usage, and/or approval of a quota increase request. You can request a quota increase for your account by following the steps below: - If a quota is marked as Yes in the Adjustable through Service Quotas column in the following [tables, you can adjust it by following the steps at Requesting a Quota Increase in the Service](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html) _[Quotas User Guide in the Service Quotas User Guide.](https://docs.aws.amazon.com/servicequotas/latest/userguide/)_ - Some quotas are marked as No in the Adjustable through Service Quotas column in the following tables. This means that they are not adjustable. To request an exception: - To request a quota increase for a Runtime quota, contact your AWS account manager. If you don't have an AWS account manager, you can't increase your quota at this time. [• To request other quota increases, submit a request through the limit increase form to be](https://console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase) considered for an increase. Note Due to overwhelming demand, priority will be given to customers who generate traffic that consumes their existing quota allocation. Your request might be denied if you don't meet this condition. Select a topic to learn more about the default global quotas for it. All global and Regional quotas are the same unless otherwise specified. 1565\n",
      "==============\n",
      "None\n",
      "text search started\n",
      "==============\n",
      "Amazon Titan Image Generator G1 models use the output image size and quality to determine how\n",
      "an image is priced. Amazon Titan Image Generator G1 models have two pricing segments based\n",
      "on size: one for 512*512 images and another for 1024*1024 images. Pricing is based on image size\n",
      "height*width, less than or equal to 512*512 or greater than 512*512.  \n",
      "[For more information on Amazon Bedrock pricing, see Amazon Bedrock Pricing.](https://aws.amazon.com/bedrock/pricing/)  \n",
      "Output 997 (Score: 5)\n",
      "\n",
      "\n",
      "chunking [uses a foundation model. View Amazon Bedrock pricing for information on the cost](https://aws.amazon.com/bedrock/pricing/) of foundation models. - No chunking: Each document is treated as a single text chunk. You might want to pre-process your documents by splitting them into separate files. Create a knowledge base 546 Note You can’t change the chunking strategy after you have created the data source. You can choose to use Amazon Bedrock’s foundation model for parsing documents to parse more than standard text. You can parse tabular data within documents with their [structure intact, for example. View Amazon Bedrock pricing for information on the cost](https://aws.amazon.com/bedrock/pricing/) of foundation models. You can choose to use an AWS Lambda function to customize your chunking strategy and how your document metadata attributes/fields are treated and ingested. Provide the Amazon S3 bucket location for the Lambda function input and output. d. Select Next. 6. On the (Score: 4)\n",
      "\n",
      "\n",
      "Throughput refers to the number and rate of inputs and outputs that a model processes and returns. You can purchase Provisioned Throughput to provision a higher level of throughput for a model at a fixed cost. If you customized a model, you must purchase Provisioned Throughput to be able to use it. You're billed hourly for a Provisioned Throughput that you purchase. For detailed information [about pricing, see Amazon Bedrock Pricing. The price per hour depends on the following factors:](https://aws.amazon.com/bedrock/pricing) 1. The model that you choose (for custom models, pricing is the same as the base model that it was customized from). 2. The number of Model Units (MUs) that you specify for the Provisioned Throughput. An MU delivers a specific throughput level for the specified model. The throughput level of an MU specifies the following: - The number of input tokens that an MU can process across all requests within a span of one minute. - The number of output tokens that an MU (Score: 4)\n",
      "\n",
      "\n",
      "AWS](https://aws.amazon.com/cloudtrail/pricing/) [CloudTrail Pricing.](https://aws.amazon.com/cloudtrail/pricing/) (Score: 3)\n",
      "\n",
      "\n",
      "more randomness in the generation. |Minimum|Maximum|Default| |---|---|---| |1.1|10.0|8.0| - The following parameters define the size that you want the output image to be. For more details [about pricing by image size, see Amazon Bedrock pricing.](https://aws.amazon.com/bedrock/pricing/) - height (Optional) – The height of the image in pixels. The default value is 1408. - width (Optional) – The width of the image in pixels. The default value is 1408. The following sizes are permissible. |Width|Height|Aspect ratio|Price equivalent to| |---|---|---|---| |1024|1024|1:1|1024 x 1024| |768|768|1:1|512 x 512| |512|512|1:1|512 x 512| |768|1152|2:3|1024 x 1024| |384|576|2:3|512 x 512| Amazon Titan models 84 |Width|Height|Aspect ratio|Price equivalent to| |---|---|---|---| |1152|768|3:2|1024 x 1024| |576|384|3:2|512 x 512| |768|1280|3:5|1024 x 1024| |384|640|3:5|512 x 512| |1280|768|5:3|1024 x 1024| |640|384|5:3|512 x 512| |896|1152|7:9|1024 x 1024| |448|576|7:9|512 x 512| |1152|896|9:7|1024 x (Score: 3)\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (찾을 수 없는 정보)\n",
    "question = \"Bedrock의 가격 정책\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "print(context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"vector\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def global_search_dev(question, graph, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        sys_prompt_template = \"\"\"\n",
    "            당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 벡터 검색으로 문서를 찾아내기에 적합한 질문을 만들어주세요.({language})\n",
    "            \n",
    "            주의: \n",
    "\n",
    "            - 질문을 고려하여 가장 관련성 높고 특징적인 질문을 생성하세요.\n",
    "            - 사용자의 질문 의도를 잘 반영하는 자연어 질문을 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#\\n\\n#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_level_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=region_name)\n",
    "        question_embedding = embeddings.embed_query(keywords)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "        CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "        WITH DISTINCT node, score\n",
    "        WHERE node:Content\n",
    "        RETURN node.text AS text, score\n",
    "        ORDER BY score DESC\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 핵심 키워드를 2개 추출합니다.\n",
    "        키워드는 다음 조건을 반드시 만족해야 합니다:\n",
    "        1. 키워드에 '_', '-' 등의 특수 문자 포함 금지 (예: custom_model 대신 custom model로 응답)\n",
    "        2. 문서 이름에 이미 포함된 내용보다는 검색하려는 특정 기능 및 개념을 잘 나타내는 구체적 단어를 선택\n",
    "        3. {language} 키워드 제공 \n",
    "        \n",
    "        주의: \n",
    "        - 너무 일반적인 단어보다는 서비스 이름과 서비스의 특정 내용을 잘 나타내는 단어를 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        model_id = low_level_model \n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt, model_id)\n",
    "        print(keywords)\n",
    "        \n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query =\"\"\"\n",
    "            CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "            WHERE node:Content\n",
    "            OPTIONAL MATCH (title:Title)-[:HAS_CONTENTS]->(node)\n",
    "            RETURN node.text as text, score, title.name as title_name, title.level as title_level\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"\n",
    "            MATCH (content:Content)\n",
    "            WITH content, $keywords AS keyword\n",
    "            WHERE content.text CONTAINS keyword\n",
    "            OPTIONAL MATCH (title:Title)-[:HAS_CONTENTS]->(content)\n",
    "            RETURN content.text AS text, \n",
    "                size(split(toLower(content.text), toLower(keyword))) - 1 AS score,\n",
    "                {\n",
    "                    title: title.name,\n",
    "                    level: title.level,\n",
    "                    value: title.value\n",
    "                } AS metadata\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"k\": k, \"keywords\": keywords}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_community/vectorstores/neo4j_vector.py:531: ExperimentalWarning: The configuration may change in the future.\n",
      "  self._driver.verify_connectivity()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics - Features of Amazon Bedrock - Amazon Bedrock pricing - Supported AWS Regions - Key definitions (Score: 0.8607656359672546)\n",
      "\n",
      "\n",
      "Charges for Guardrails for Amazon Bedrock will be incurred only for the policies configured in the\n",
      "[guardrail. The price for each policy type is available at Amazon Bedrock Pricing. If guardrails blocks](https://aws.amazon.com/bedrock/pricing/)\n",
      "the input prompt, you will be charged for the guardrail evaluation. There will be no charges for\n",
      "foundation model inference calls. If guardrails blocks the model response, you will be charged for\n",
      "guardrails evaluation of the input prompt and the model response. In this case, you will be charged\n",
      "for the foundation model inference calls as well the model response that was generated prior to\n",
      "guardrails evaluation.  \n",
      "How charges are calculated 358 (Score: 0.8290265798568726)\n",
      "\n",
      "\n",
      "What is Amazon Bedrock? 1 Features of Amazon Bedrock .. 1 Amazon Bedrock pricing 2 Supported AWS Regions 2 Key definitions ... 5 Basic concepts ... 5 Advanced features . 7 Getting started ... 8 Request access to an Amazon Bedrock foundation model .... 11 (Optional tutorials) Explore Amazon Bedrock features through the console or API . 12 Getting started in the console .. 12 Explore the text playground . 13 Explore the image playground .. 13 Getting started with the API 14 Install the AWS CLI or an AWS SDK .. 14 Get credentials to grant programmatic access to a user .. 15 Try out some Amazon Bedrock API requests .. 16 Run examples with the AWS CLI ... 17 Run examples with the AWS SDK for Python (Boto3) . 19 Run examples with a SageMaker notebook .... 23 Working with AWS SDKs .. 26 Manage model access . 28 Modify model access .... 29 Control model access permissions . 30 Foundation model information 33 Using foundation models . 37 Get model information 38 Model support by AWS Region . 39 (Score: 0.8269864320755005)\n",
      "\n",
      "\n",
      "When you sign up for AWS, your AWS account is automatically signed up for all services in AWS, including Amazon Bedrock. However, you are charged only for the services that you use. [To see your bill, go to the Billing and Cost Management Dashboard in the AWS Billing and Cost](https://console.aws.amazon.com/billing/) [Management console. To learn more about AWS account billing, see the AWS Billing User Guide. If](https://console.aws.amazon.com/billing/) [you have questions concerning AWS billing and AWS accounts, contact AWS Support.](https://aws.amazon.com/contact-us/) With Amazon Bedrock, you pay to run inference on any of the third-party foundation models. Pricing is based on the volume of input tokens and output tokens, and on whether you have [purchased provisioned throughput for the model. For more information, see the Model providers](https://console.aws.amazon.com/bedrock/home#/providers) page in the Amazon Bedrock console. For each model, pricing is listed following the model (Score: 0.821861207485199)\n",
      "\n",
      "\n",
      "version. For more information about purchasing Provisioned Throughput, see Provisioned Throughput for Amazon Bedrock. [For more information, see Amazon Bedrock Pricing.](https://aws.amazon.com/bedrock/pricing) (Score: 0.8147680759429932)\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (찾을 수 없는 정보)\n",
    "question = \"Amazon Bedrock의 가격 정책\"\n",
    "\n",
    "content = global_search_dev(question, graph, 'English', 5)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate_answer_dev(question, context):\n",
    "    # Prompt setting\n",
    "    sys_prompt_template = \"당신은 AWS에 정통한 전문 엔지니어입니다. 주어진 사전 정보만 활용하여, 사용자 질문에 답변을 생성하세요. 사전 정보로 주어지지 않은 내용에 대한 질문에는 모른다고 답변하세요.\"\n",
    "    usr_prompt_template = \"#사전 정보: {context}\\n\\n #사용자 질문:\\n {question}\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", sys_prompt_template), (\"human\",usr_prompt_template)])\n",
    "\n",
    "    # Model setting\n",
    "    model_kwargs = {\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    llm = ChatBedrock(model_id=high_level_model, region_name=\"us-west-2\", model_kwargs=model_kwargs, streaming=True)   \n",
    "\n",
    "    # Output setting\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | parser\n",
    "    for chunk in chain.stream({\"context\": context, \"question\": question}):\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1359]\n",
      "Traversing 'Agents for Amazon Bedrock'...\n",
      "target_node: [1521]\n",
      "Traversing 'Manage an Amazon Bedrock agent'...\n",
      "target_node: [1565]\n",
      "Traversing 'Manage agent memory'...\n",
      "target_node: [1575]\n",
      "Traversing 'Disable memory for your Amazon Bedrock agent'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 2\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "None\n",
      "vector search started\n",
      "Bedrock에서 Agent의 성능을 최적화하는 방법:\n",
      "\n",
      "agent memory size, agent batch size, agent learning rate, agent reward shaping, agent exploration strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_community/vectorstores/neo4j_vector.py:531: ExperimentalWarning: The configuration may change in the future.\n",
      "  self._driver.verify_connectivity()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 정보에 따르면, Agents for Amazon Bedrock에는 특정 사용 사례에 대한 성능 최적화 옵션이 제공됩니다. 단일 지식베이스를 사용하는 간단한 사용 사례의 경우, 레이턴시 최적화를 위한 다양한 흐름을 선택할 수 있습니다. \n",
      "\n",
      "성능 최적화를 위해서는 다음과 같은 조건이 충족되어야 합니다:\n",
      "\n",
      "1. 에이전트에 단일 지식베이스만 포함되어 있어야 합니다.\n",
      "2. 에이전트에 액션 그룹이 없거나 모두 비활성화되어 있어야 합니다.\n",
      "3. 에이전트가 사용자로부터 추가 정보를 요청하지 않아야 합니다.\n",
      "4. 에이전트가 기본 오케스트레이션 프롬프트 템플릿을 사용해야 합니다.\n",
      "\n",
      "이러한 조건을 충족하는지 확인하는 방법은 콘솔 또는 API를 통해 에이전트를 관리하는 방법에 따라 다릅니다. 콘솔에서는 에이전트 페이지에서 이 정보를 확인할 수 있으며, API를 통해서는 CreateAgent 요청에서 관련 필드를 확인할 수 있습니다."
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 \n",
    "question = \"Bedrock에서 Agent의 성능을 최적화하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1070]\n",
      "Traversing 'Knowledge bases for Amazon Bedrock'...\n",
      "target_node: [1284]\n",
      "Traversing 'Test a knowledge base in Amazon Bedrock'...\n",
      "target_node: [1286]\n",
      "Traversing 'Query the knowledge base and return results or generate responses'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 9\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "Partial\n",
      "Bedrock에서 Knowledge Base를 테스트하는 방법은 다음과 같습니다:\n",
      "\n",
      "1. AWS Management Console에 로그인하고 Bedrock 콘솔(https://console.aws.amazon.com/bedrock/)로 이동합니다.\n",
      "2. 왼쪽 탐색 창에서 Knowledge bases를 선택합니다.\n",
      "3. Knowledge bases 섹션에서 다음 중 하나를 수행합니다:\n",
      "   - 테스트하려는 Knowledge Base 옆의 라디오 버튼을 선택하고 Test knowledge base를 클릭합니다. 오른쪽에서 테스트 창이 열립니다.\n",
      "   - 테스트하려는 Knowledge Base를 선택합니다. 오른쪽에서 테스트 창이 열립니다.\n",
      "4. Generate responses for your query 옵션을 선택하거나 선택 해제합니다:\n",
      "   - 지식 베이스에서 직접 검색된 정보를 반환하려면 Generate responses를 끕니다. Bedrock은 관련 텍스트 청크를 반환합니다.\n",
      "   - 지식 베이스에서 검색된 정보를 기반으로 응답을 생성하려면 Generate responses를 켭니다. Bedrock은 데이터 소스를 기반으로 응답을 생성하고 정보 출처를 각주로 표시합니다.\n",
      "5. Generate responses를 켠 경우 Select model을 선택하여 사용할 모델을 선택한 후 Apply를 클릭합니다.\n",
      "6. (선택 사항) 구성 아이콘을 클릭하여 검색 유형, 최대 검색 결과 수, 필터, 프롬프트 템플릿, 가드레일 등의 구성을 수정할 수 있습니다.\n",
      "7. 채팅 창에 쿼리를 입력하고 Run을 클릭하여 지식 베이스에서 응답을 받습니다.\n",
      "8. 응답을 검토하고 각주를 클릭하여 소스 파일의 발췌문을 확인하거나 소스 세부 정보를 확인할 수 있습니다."
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Knowledge Base를 테스트하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [3333]\n",
      "Traversing 'Quotas for Amazon Bedrock'...\n",
      "target_node: [3346]\n",
      "Traversing 'Model inference prompt quotas'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 2\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "None\n",
      "vector search started\n",
      "Bedrock의 가격 정책에 대한 질문:\n",
      "\n",
      "on-demand pricing, pay-as-you-go, no upfront costs, pricing based on usage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_community/vectorstores/neo4j_vector.py:531: ExperimentalWarning: The configuration may change in the future.\n",
      "  self._driver.verify_connectivity()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 정보에 따르면 Amazon Bedrock의 가격 정책은 다음과 같습니다:\n",
      "\n",
      "1. 기본 가격 정책:\n",
      "   - Amazon Bedrock 사용 시 입력 토큰과 출력 토큰의 볼륨에 따라 과금됩니다.\n",
      "   - 모델 제공업체별로 가격이 다르게 책정되어 있습니다.\n",
      "\n",
      "2. 프로비저닝된 처리량(Provisioned Throughput) 옵션:\n",
      "   - 프로비저닝된 처리량을 구매하면 더 저렴한 가격으로 사용할 수 있습니다.\n",
      "   - 프로비저닝된 처리량에 대한 자세한 내용은 \"Provisioned Throughput for Amazon Bedrock\" 문서를 참고하세요.\n",
      "\n",
      "3. Guardrails 사용 시 과금:\n",
      "   - Guardrails 정책 구성에 따라 과금이 발생합니다.\n",
      "   - Guardrails에서 입력 프롬프트를 차단하는 경우 Guardrails 평가 비용이 청구됩니다.\n",
      "   - Guardrails에서 모델 응답을 차단하는 경우 Guardrails 평가 비용과 모델 추론 비용이 청구됩니다.\n",
      "\n",
      "4. AWS 계정 및 청구:\n",
      "   - AWS 계정에 자동으로 모든 AWS 서비스가 등록되지만, 실제로 사용한 서비스에 대해서만 과금됩니다.\n",
      "   - AWS 청구 및 계정 관련 문의는 AWS 지원팀에 문의하시기 바랍니다."
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (문서에 없는 정보)\n",
    "question = \"Bedrock의 가격 정책\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [19]\n",
      "Traversing 'What is Amazon Bedrock?'...\n",
      "target_node: [22]\n",
      "Traversing 'Features of Amazon Bedrock'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 2\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "Complete\n",
      "Amazon Bedrock Playground is a feature of the Amazon Bedrock service that allows you to experiment with prompts and configurations for foundation models. The Bedrock Playground provides a graphical interface in the AWS console where you can:\n",
      "\n",
      "- Run model inference by sending prompts to different foundation models and see the generated responses.\n",
      "- Explore how to use the API to make requests to the InvokeModel APIs for your application.\n",
      "- Experiment with different prompts and configurations to understand the capabilities of the foundation models.\n",
      "\n",
      "The Bedrock Playground lets you test and explore the features of Amazon Bedrock without having to set up your own application. This can be a useful way to get familiar with the service and determine the best foundation model and configurations for your use case before integrating it into your production application."
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"What is Amazon Bedrock Playground?\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'node_level_search':\n",
    "    context.contents = node_level_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "status = check_relevance_dev(question, context.contents, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)\n",
    "\n",
    "if status == 'Partial':\n",
    "    context.contents = get_sibling_contents_dev(graph, context.parent_id, context.contents, context.k)\n",
    "elif status == 'None':\n",
    "    context.contents = subgraph_level_search_dev(question, graph, subgraph, 'English', 5)\n",
    "\n",
    "generate_answer_dev(question, context.contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region_name = \"us-east-1\"\n",
    "#llm_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "llm_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "def converse_with_bedrock(sys_prompt, usr_prompt):\n",
    "    temperature = 0\n",
    "    top_p = 0.1\n",
    "    top_k = 1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=llm_model, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "\n",
    "def create_prompt(sys_template, user_template, **kwargs):\n",
    "    sys_prompt = [{\"text\": sys_template.format(**kwargs)}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template.format(**kwargs)}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "boto3_client = init_boto3_client(region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgraph_dev(question, graph):\n",
    "    question = question\n",
    "    query = \"\"\"\n",
    "        MATCH (n:Title {level: \"1\"})\n",
    "        RETURN n.value, id(n) as node_id\n",
    "    \"\"\"\n",
    "    results = graph.run(query)\n",
    "    subgraph_list = [(record[\"n.value\"], record[\"node_id\"]) for record in results]\n",
    "    subgraph_list_with_number = [f\"{i}. {subgraph[0]}\" for i, subgraph in enumerate(subgraph_list)]\n",
    "\n",
    "    sys_prompt_template = \"\"\" \n",
    "    You are an expert engineer well-versed in AWS manual documents. \n",
    "    Your task is to select the most appropriate manual document name for the user's question. \n",
    "    If there are no relevant documents, provide an empty list (\"\"). \"\"\"\n",
    "\n",
    "    usr_prompt_template = \"\"\" \n",
    "    Please select the single most relevant document name for the given question.\n",
    "\n",
    "    #Question: {question}\n",
    "\n",
    "    #Document List: {subgraph_list_with_number}\n",
    "\n",
    "    #Response Format: Provide only the index number of the selected document (omit any preamble) \"\"\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, question=question, subgraph_list_with_number=subgraph_list_with_number)\n",
    "    \n",
    "    selected_id = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    try:\n",
    "        if selected_id == \"\":\n",
    "            return [], \"\", \"generate_answer\"\n",
    "\n",
    "        else: \n",
    "            selected_subgraph_id = subgraph_list[int(selected_id)][1]\n",
    "            print(\"Selected:\", subgraph_list[int(selected_id)][0])\n",
    "            return [selected_subgraph_id], subgraph_list[int(selected_id)][0], \"traverse_child\"\n",
    "    except:\n",
    "        return [], \"\", \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 제공하는 모델 목록\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] |  | generate_answer\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (현재 없는 내용)\n",
    "question = \"SageMaker에서 모델을 배포하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraverseResult:\n",
    "    def __init__(self, parent_id, parent_name, child_level, selected_child_ids, child_names, next_action):\n",
    "        self.parent_id = parent_id\n",
    "        self.parent_name = parent_name\n",
    "        self.child_level = child_level\n",
    "        self.selected_child_ids = selected_child_ids\n",
    "        self.child_names = child_names\n",
    "        self.next_action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def traverse_child_dev(question, subgraph, graph, target_node):\n",
    "    parent_id = target_node[0]\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE id(n) = $parent_id\n",
    "        OPTIONAL MATCH (n)-[:HAS_CHILD]->(c)\n",
    "        RETURN n.value as parent_name, c.level as child_level, c.value as child_name, id(c) as child_id\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    query_results = graph.run(query, params)\n",
    "\n",
    "    parent_name = None\n",
    "    child_level = None\n",
    "    child_list = []\n",
    "    child_names = []\n",
    "\n",
    "    for record in query_results:\n",
    "        if parent_name is None:\n",
    "            parent_name = record[\"parent_name\"]\n",
    "        if child_level is None:\n",
    "            child_level = record[\"child_level\"]\n",
    "        if record[\"child_name\"] is not None:\n",
    "            child_list.append((record[\"child_name\"], record[\"child_id\"]))\n",
    "            child_names.append(record[\"child_name\"])\n",
    "\n",
    "    print(f\"Traversing '{parent_name}'...\")\n",
    "\n",
    "    if not child_list:\n",
    "        print(\"No child. Proceed to 'get_contents'...\")\n",
    "        #print(f\"Debug: {parent_id}, {parent_name}, {child_level}, [], [], 'get_contents'\")\n",
    "        return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")\n",
    "\n",
    "    child_list_with_number = [f\"{i}. {child}\" for i, child in enumerate(child_list)]\n",
    "    sys_prompt_template = \"\"\"\n",
    "    당신은 AWS 매뉴얼 문서에 정통한 전문 엔지니어입니다.\n",
    "    당신의 임무는 사용자의 질문에 답변하기 위해, <{subgraph}> 매뉴얼 문서에서 가장 관련성 높은 하위 메뉴를 선택하는 것입니다.\n",
    "\n",
    "    작업 순서:\n",
    "    1. 주어진 하위 메뉴 목록을 검토하여 직접적으로 연관된 메뉴들을 찾습니다.\n",
    "    2. 연관성이 가장 높은 메뉴를 선택하여, 인덱스 번호(0부터 시작)로 응답합니다.\n",
    "    3. 질문과 매우 밀접한 메뉴가 1개 이상인 경우, 선택한 메뉴의 인덱스 번호 목록으로 응답합니다.\n",
    "\n",
    "    선택 기준:\n",
    "    - 질문의 핵심 키워드와 일치하고 질문의 맥락에 맞는 메뉴를 우선적으로 고려하세요.\n",
    "    - 일반적 가이드보다는 질문의 특정 주제나 기능을 다루는 메뉴를 선호합니다. 예를 들어, 'Getting started' 가이드보다는 특정 기능이나 서비스에 대한 상세 설명이 있는 항목을 선호합니다.\n",
    "    - 반드시 선택을 해야하는 것은 아닙니다. 연관성이 낮거나 불확실한 메뉴는 선택하지 마세요.\n",
    "\n",
    "    \"\"\"\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #질문: {question}\n",
    "\n",
    "    #메뉴 목록:\n",
    "    {child_list_with_number}\n",
    "\n",
    "    #응답 형식: {csv_list_response_format}.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, subgraph=subgraph, question=question, child_list_with_number=child_list_with_number, csv_list_response_format=csv_list_response_format)\n",
    "    selected_ids = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "\n",
    "    try:\n",
    "        selected_id_list = [int(id.strip()) for id in selected_ids.split(',') if id.strip().isdigit()]\n",
    "\n",
    "        if not selected_id_list:\n",
    "            #print(f\"Debug1: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "            return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")\n",
    "        \n",
    "        selected_child_ids = [child_list[id][1] for id in selected_id_list if id < len(child_list)]\n",
    "        selected_child_names = [child_list[id][0] for id in selected_id_list if id < len(child_list)]\n",
    "\n",
    "        #print(f\"Debug2: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "        return TraverseResult(parent_id, parent_name, child_level, selected_child_ids, selected_child_names, \"traverse_child\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        #print(f\"Debug3: Exception occurred: {str(e)}\")\n",
    "        #print(f\"Debug4: {parent_id}, {parent_name}, {child_level}, {selected_child_ids}, {selected_child_names}, 'traverse_child'\")\n",
    "        return TraverseResult(parent_id, parent_name, -1, [], [], \"get_contents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, parent_id, parent_name, contents, contents_length, search_type, k):\n",
    "        self.parent_id = parent_id\n",
    "        self.parent_name = parent_name\n",
    "        self.contents = contents\n",
    "        self.contents_length = contents_length\n",
    "        self.search_type = search_type\n",
    "        self.k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1908]\n",
      "Traversing 'Custom models'...\n",
      "target_node: [2027]\n",
      "Traversing 'Code samples for model customization'...\n",
      "No child. Proceed to 'get_contents'...\n"
     ]
    }
   ],
   "source": [
    "traverse_results = []\n",
    "\n",
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Custom Model 활용 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_dev(graph, parent_id, k=5):\n",
    "    count_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id\n",
    "        RETURN count(c) as contents_length, n.value as parent_name\n",
    "    \"\"\"\n",
    "    params = {\"parent_id\": parent_id}\n",
    "    count_result = graph.run(count_query, params).data()[0]\n",
    "    contents_length = count_result['contents_length']\n",
    "    parent_name = count_result['parent_name']\n",
    "    print(f\"Num Documents: {contents_length}\")\n",
    "\n",
    "    if contents_length <= k * 2:\n",
    "        search_type = \"get_short_documents\"\n",
    "        content_query = \"\"\"\n",
    "            MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "            WHERE id(n) = $parent_id\n",
    "            RETURN c.text\n",
    "            ORDER BY c.order\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "        params = {\"parent_id\": parent_id, \"k\": k}\n",
    "        content_results = graph.run(content_query, params)\n",
    "        contents = [record[\"c.text\"] for record in content_results]\n",
    "        context = \" \".join(contents)\n",
    "\n",
    "    else:\n",
    "        search_type = \"local_search\"\n",
    "        context = \"\"\n",
    "\n",
    "    return Context(parent_id, parent_name, context, contents_length, search_type, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [116]\n",
      "Traversing 'Supported foundation models in Amazon Bedrock'...\n",
      "target_node: [149]\n",
      "Traversing 'Amazon Bedrock model IDs'...\n",
      "target_node: [152]\n",
      "Traversing 'Amazon Bedrock base model IDs (on-demand throughput)'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 4\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "The following is a list of model IDs for the currently available base models. You use a model ID through the API to identify the base model that you want to use with on-demand [throughput, such as in a InvokeModel request, or that you want to customize, such as in a](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) [CreateModelCustomizationJob request.](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateModelCustomizationJob.html) Note You should regularly check the Model lifecycle page for information about model deprecation and update model IDs as necessary. Once a model has reached end-of-life, the model ID no longer works. |Provider|Model name|Version|Model ID| |---|---|---|---| |AI21 Labs|Jamba-Instruct|1.x|ai21.jamba-instruct- v1:0| |AI21 Labs|Jurassic-2 Mid|1.x|ai21.j2-mid-v1| |AI21 Labs|Jurassic-2 Ultra|1.x|ai21.j2-ultra-v1| |Amazon|Titan Text G1 - Express|1.x|amazon.titan-text- express-v1| |Amazon|Titan Text G1 - Lite|1.x|amazon.titan-text- lite-v1| |Amazon|Titan Text Premier|1.x|amazon.titan-text- premier-v1:0| |Amazon|Titan Embeddings G1 - Text|1.x|amazon.titan-embed- text-v1| Base models IDs (on-demand) 56 |Provider|Model name|Version|Model ID| |---|---|---|---| |Amazon|Titan Embedding Text v2|1.x|amazon.titan-embed- text-v2:0| |Amazon|Titan Multimodal Embeddings G1|1.x|amazon.titan-embed- image-v1| |Amazon|Titan Image Generator G1 V1|1.x|amazon.titan-image- generator-v1| |Amazon|Titan Image Generator G1 V2|2.x|amazon.titan-image- generator-v2:0| |Anthropic|Claude|2.0|anthropic.claude-v2| |Anthropic|Claude|2.1|anthropic.claude-v2:1| |Anthropic|Claude 3 Sonnet|1.0|anthropic.claude-3- sonnet-20240229-v 1:0| |Anthropic|Claude 3.5 Sonnet|1.0|anthropic.claude-3-5- sonnet-20240620- v1:0| |Anthropic|Claude 3 Haiku|1.0|anthropic.claude-3- haiku-20240307-v1:0| |Anthropic|Claude 3 Opus|1.0|anthropic.claude-3- opus-20240229-v1:0| |Anthropic|Claude Instant|1.x|anthropic.claude-i nstant-v1| |Cohere|Command|14.x|cohere.command-tex t-v14| |Cohere|Command Light|15.x|cohere.command-lig ht-text-v14| Base models IDs (on-demand) 57 |Provider|Model name|Version|Model ID| |---|---|---|---| |Cohere|Command R|1.x|cohere.command-r-v 1:0| |Cohere|Command R+|1.x|cohere.command-r-p lus-v1:0| |Cohere|Embed English|3.x|cohere.embed-engli sh-v3| |Cohere|Embed Multilingual|3.x|cohere.embed-multi lingual-v3| |Meta|Llama 2 Chat 13B|1.x|meta.llama2-13b-ch at-v1| |Meta|Llama 2 Chat 70B|1.x|meta.llama2-70b-ch at-v1| |Meta|Llama 3 8B Instruct|1.x|meta.llama3-8b-ins truct-v1:0| |Meta|Llama 3 70B Instruct|1.x|meta.llama3-70b-in struct-v1:0| |Meta|Llama 3.1 8B Instruct|1.x|meta.llama3-1-8b-i nstruct-v1:0| |Meta|Llama 3.1 70B Instruct|1.x|meta.llama3-1-70b- instruct-v1:0| |Meta|Llama 3.1 405B Instruct|1.x|meta.llama3-1-405b- instruct-v1:0| |Mistral AI|Mistral 7B Instruct|0.x|mistral.mistral-7b- instruct-v0:2| |Mistral AI|Mixtral 8X7B Instruct|0.x|mistral.mixtral-8x7b- instruct-v0:1| Base models IDs (on-demand) 58 |Provider|Model name|Version|Model ID| |---|---|---|---| |Mistral AI|Mistral Large|1.x|mistral.mistral-la rge-2402-v1:0| |Mistral AI|Mistral Large 2 (24.07)|1.x|mistral.mistral-la rge-2407-v1:0| |Mistral AI|Mistral Small|1.x|mistral.mistral-sm all-2402-v1:0| |Stability AI|Stable Diffusion XL|0.x|stability.stable-d iffusion-xl-v0| |Stability AI|Stable Diffusion XL|1.x|stability.stable-d iffusion-xl-v1|\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 활용가능한 모델 목록\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [2672]\n",
      "Traversing 'Code examples for Amazon Bedrock using AWS SDKs'...\n",
      "target_node: [3227]\n",
      "Traversing 'Code examples for Agents for Amazon Bedrock using AWS SDKs'...\n",
      "target_node: [3276]\n",
      "Traversing 'Scenarios for Agents for Amazon Bedrock using AWS SDKs'...\n",
      "Num Documents: 1\n",
      "==============\n",
      "Search Type: get_short_documents\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock의 Agent에서 SDK로 Action을 정의하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "\n",
    "searching_scheme = \"full_text\" # full_text | keyword | vector\n",
    "csv_list_response_format = \"Your response should be a list of comma separated values, eg: `foo, bar` or `foo,bar`\"\n",
    "\n",
    "def local_search_dev(question, graph, parent_id, parent_name, language = \"English\", k=5):\n",
    "\n",
    "    if searching_scheme == \"vector\":\n",
    "        index_name = \"content_embedding_index\"\n",
    "\n",
    "        embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", region_name=\"us-east-1\")\n",
    "        question_embedding = embeddings.embed_query(question)\n",
    "\n",
    "        vector_search_query = \"\"\"\n",
    "            MATCH (parent)-[:HAS_CONTENTS]->(child:Content)\n",
    "            WHERE id(parent) = $parent_id\n",
    "\n",
    "            CALL db.index.vector.queryNodes($index_name, $k, $question_embedding) YIELD node, score\n",
    "            WHERE node = child\n",
    "\n",
    "            RETURN node.text AS text, score, {} AS metadata\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $k\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"question_embedding\": question_embedding,\n",
    "            \"k\": k,\n",
    "            \"index_name\": index_name\n",
    "        }\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "            node_label=\"Content\",\n",
    "            text_node_property=\"text\", \n",
    "            embedding_node_property=\"embedding\"\n",
    "        )\n",
    "\n",
    "        search_results = vector_store.query(vector_search_query, params=params)\n",
    "\n",
    "    else:\n",
    "        sys_prompt_template = \"\"\"\n",
    "        당신은 AWS에 정통한 전문 엔지니어입니다. 사용자의 질문을 바탕으로 매뉴얼에서 핵심 키워드를 1개 추출합니다.\n",
    "        키워드는 다음 조건을 반드시 만족해야 합니다:\n",
    "        1. 키워드에 '_', '-' 등의 특수 문자 포함 금지 (예: custom_model 대신 custom model로 응답)\n",
    "        2. 주어진 문서 이름 내에서 질문의 맥락에 가장 적합한 단어를 선택 (문서 이름은 키워드에 포함할 필요가 없음)\n",
    "        2. 문서 이름에 이미 포함된 내용보다는 검색하려는 특정 기능 및 개념을 잘 나타내는 구체적 단어를 선택\n",
    "        3. {language} 키워드 제공 \n",
    "        \n",
    "        주의: \n",
    "        - 문서 이름과 질문을 고려하여 해당 문서 내에서 가장 관련성 높고 특징적인 단어를 선택하세요.\n",
    "        - 너무 일반적인 단어보다는 문서의 특정 내용을 잘 나타내는 단어를 선호합니다.\n",
    "        \"\"\"\n",
    "        usr_prompt_template = \"#검색 대상 문서 이름:\\n{parent_name}\\n\\n#질문:\\n{question}\\n\\n #응답 형식:\\n{csv_list_response_format}\"\n",
    "\n",
    "        sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, language=language, parent_name=parent_name, question=question, csv_list_response_format=csv_list_response_format)\n",
    "        keywords = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    \n",
    "\n",
    "        if searching_scheme == \"full_text\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child\n",
    "                CALL db.index.fulltext.queryNodes(\"Search_Content_by_FullText\", $keywords) YIELD node, score\n",
    "                WHERE node = child\n",
    "                RETURN node.text as text, score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        elif searching_scheme == \"keyword\":\n",
    "            search_query = \"\"\"MATCH (parent)-[:HAS_CONTENTS]->(child)\n",
    "                WHERE id(parent) = $parent_id\n",
    "                WITH child, $keywords AS keyword\n",
    "                WHERE child.text CONTAINS keyword\n",
    "                RETURN child.text AS text, \n",
    "                    size(split(toLower(child.text), toLower(keyword))) - 1 AS score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "            \"\"\"\n",
    "        \n",
    "        params = {\"parent_id\": parent_id, \"keywords\": keywords, \"k\": k}\n",
    "        search_results = graph.run(search_query, params)\n",
    "        \n",
    "    content = \"\\n\\n\\n\".join(f\"{record['text']} (Score: {record['score']})\" for record in search_results)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [2672]\n",
      "Traversing 'Code examples for Amazon Bedrock using AWS SDKs'...\n",
      "target_node: [3227]\n",
      "Traversing 'Code examples for Agents for Amazon Bedrock using AWS SDKs'...\n",
      "target_node: [3276]\n",
      "Traversing 'Scenarios for Agents for Amazon Bedrock using AWS SDKs'...\n",
      "Num Documents: 1\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "file, replacing ${training-bucket} and ${output-bucket} with your S3 bucket names. --- { \"trainingDataConfig\": { \"s3Uri\": \"s3://${training-bucket}/train.jsonl\" }, \"outputDataConfig\": { \"s3Uri\": \"s3://${output-bucket}\" } } --- To submit a model customization job, navigate to the folder containing --- FineTuningData.json in a terminal and run the following command in the command line, --- replacing ${your-customization-role-arn} with the model customization role that you set up. --- aws bedrock create-model-customization-job \\ --customization-type FINE_TUNING \\ --base-model-identifier arn:aws:bedrock:us-east-1::foundation-model/ amazon.titan-text-express-v1 \\ --role-arn ${your-customization-role-arn} \\ --job-name MyFineTuningJob \\ --custom-model-name MyCustomModel \\ --hyper-parameters epochCount=1,batchSize=1,learningRate=.0005,learningRateWarmupSteps=0 \\ --cli-input-json file://FineTuningData.json --- The response returns a jobArn. Allow the job some time to complete. You can check its (Score: 7.889132499694824)\n",
      "\n",
      "\n",
      "created. To check its status, provide the name or ARN of the provisioned model as the --- provisioned-model-id in the following command. --- aws bedrock get-provisioned-model-throughput \\ --provisioned-model-id ${provisioned-model-arn} --- --- When the status is InService, you can run inference with your custom model with the following command. You must provide the ARN of the provisioned model as the model-id. The output is written to a file named output.txt in your current folder. --- aws bedrock-runtime invoke-model \\ --model-id ${provisioned-model-arn} \\ --body '{\"inputText\": \"What is AWS?\", \"textGenerationConfig\": {\"temperature\": 0.5}}' \\ --cli-binary-format raw-in-base64-out \\ output.txt --- Code samples 954 Python Run the following code snippet to submit a fine-tuning job. Replace ${your-customization--- role-arn} with the ARN of the MyCustomizationRole that you set up and replace ${training-bucket} and ${output-bucket} with your S3 bucket names. --- import boto3 import json (Score: 6.566826820373535)\n",
      "\n",
      "\n",
      "The following code samples show how to prepare a basic dataset, set up permissions, create a custom model, view the output files, purchase throughput for the model, and run inference on the model. You can modify these code snippets to your specific use-case. 1. Prepare the training dataset. a. Create a training dataset file containing the following one line and name it train.jsonl. --- {\"prompt\": \"what is AWS\", \"completion\": \"it's Amazon Web Services\"} --- b. Create an S3 bucket for your training data and another one for your output data (the names must be unique). c. Upload train.jsonl into the training data bucket. Code samples 947 2. Create a policy to access your training and attach it to an IAM role with a Amazon Bedrock trust relationship. Select the tab corresponding to your method of choice and follow the steps. Console 1. Create the S3 policy. a. [Navigate to the IAM console at https://console.aws.amazon.com/iam and choose](https://console.aws.amazon.com/iam) Policies from the (Score: 6.3483991622924805)\n",
      "\n",
      "\n",
      "bedrock.get_provisioned_model_throughput(provisionedModelId=provisionedModelArn) --- --- When the status is InService, you can run inference with your custom model with the following command. You must provide the ARN of the provisioned model as the modelId. --- import json import logging import boto3 from botocore.exceptions import ClientError class ImageError(Exception): \"Custom exception for errors returned by the model\" def __init__(self, message): self.message = message logger = logging.getLogger(__name__) --- Code samples 956 --- logging.basicConfig(level=logging.INFO) def generate_text(model_id, body): \"\"\" Generate text using your provisioned custom model. Args: model_id (str): The model ID to use. body (str) : The request body to use. Returns: response (json): The response from the model. \"\"\" logger.info( \"Generating text with your provisioned custom model %s\", model_id) brt = boto3.client(service_name='bedrock-runtime') accept = \"application/json\" content_type = (Score: 5.813394546508789)\n",
      "\n",
      "\n",
      "status with the following command. --- aws bedrock get-model-customization-job \\ --job-identifier \"jobArn\" --- Code samples 953 When the status is COMPLETE, you can see the trainingMetrics in the response. You can download the artifacts to the current folder by running the following command, replacing --- aet.et-bucket with your output bucket name and jobId with the ID of the customization --- job (the sequence following the last slash in the jobArn). --- aws s3 cp s3://${output-bucket}/model-customization-job-jobId . --recursive --- Purchase a no-commitment Provisioned Throughput for your custom model with the following command. Note You will be charged hourly for this purchase. Use the console to see price estimates for different options. --- aws bedrock create-provisioned-model-throughput \\ --model-id MyCustomModel \\ --provisioned-model-name MyProvisionedCustomModel \\ --model-units 1 --- The response returns a provisionedModelArn. Allow the Provisioned Throughput some time to be (Score: 5.7399396896362305)\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock의 Agent에서 SDK로 Action을 정의하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "\n",
    "print(search_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevance_dev(question, content, parent_name, contents_length, search_type, k=5):\n",
    "    optional_prompt1 = \"\"\n",
    "    optional_prompt2 = \"\"\n",
    "    \n",
    "    if search_type == \"get_short_documents\" and contents_length > k:\n",
    "        optional_prompt1 = \"- 사전 정보가 질문 취지에 부합하지만 뒷 내용 추가 확인 필요: 'Partial'\"\n",
    "        optional_prompt2 = \"또는 `Partial`\"\n",
    "\n",
    "    sys_prompt_template = \"\"\"\n",
    "    당신은 AWS에 정통한 전문 엔지니어입니다. 오직 주어진 사전 정보만을 활용하여 사용자 질문에 답변 가능한지 판단하는 것이 당신의 임무입니다.\n",
    "    \n",
    "    판단 기준:\n",
    "    1. 문서 이름이 질문과 직접적으로 관련이 있는가?\n",
    "    2. 사전 정보가 질문에 대한 구체적 답변을 제공하는가?\n",
    "\n",
    "    응답 방법:\n",
    "    - 문서 이름 및 사전 정보가 질문과 관련 없음: 'None'\n",
    "    - 사전 정보만으로 질문에 답변 가능: 'Complete'\n",
    "    {partial1}\n",
    "\n",
    "    서두는 생략하고, 주어진 질문과 사전정보 및 문서이름을 바탕으로 `None` 또는 `Complete`{partial2}으로만 답변하세요.\n",
    "    \"\"\"\n",
    "    usr_prompt_template = \"\"\"\n",
    "    #사전 정보 (문서이름: {parent_name})\n",
    "    {context}\n",
    "    \n",
    "    #질문: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_prompt_template, usr_prompt_template, partial1=optional_prompt1, partial2=optional_prompt2, parent_name=parent_name, question=question, context=content)\n",
    "    status = converse_with_bedrock(sys_prompt, usr_prompt)\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [2233]\n",
      "Traversing 'Security in Amazon Bedrock'...\n",
      "target_node: [2237]\n",
      "Traversing 'Data protection'...\n",
      "target_node: [2242]\n",
      "Traversing 'Data encryption'...\n",
      "target_node: [2252]\n",
      "Traversing 'Encryption of model customization jobs and artifacts'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 22\n",
      "==============\n",
      "Search Type: local_search\n",
      "==============\n",
      "Decrypt requests to AWS KMS to generate data keys encrypted by](https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html) your customer managed key and decrypt the encrypted data keys so that they can be used to encrypt the model artifacts. [• Send CreateGrant requests to AWS KMS to create scoped down secondary grants with a subset](https://docs.aws.amazon.com/kms/latest/APIReference/API_CreateGrant.html) of the above operations (DescribeKey, GenerateDataKey, Decrypt), for the asynchronous execution of model customization, model copy, or Provisioned Throughput creation. - Amazon Bedrock specifies a retiring principal during the creation of grants, so the service can [send a RetireGrant request.](https://docs.aws.amazon.com/kms/latest/APIReference/API_RetireGrant.html) You have full access to your customer managed AWS KMS key. You can revoke access to the [grant by following the steps at Retiring and revoking grants in the AWS Key Management (Score: 6.988223075866699)\n",
      "\n",
      "\n",
      "\"keyId\": \"arn:aws:kms:us east-1:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE\" }, \"requestID\": \"ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE\", \"eventID\": \"ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE\", \"readOnly\": false, \"resources\": [ { \"accountId\": \"111122223333\", \"type\": \"AWS::KMS::Key\", \"ARN\": \"arn:aws:kms:us east-1:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE\" } ], \"eventType\": \"AwsApiCall\", \"managementEvent\": true, \"recipientAccountId\": \"111122223333\", \"eventCategory\": \"Management\" } --- Encryption of training, validation, and output data When you use Amazon Bedrock to run a model customization job, you store the input files in your Amazon S3 bucket. When the job completes, Amazon Bedrock stores the output metrics files in the S3 bucket that you specifed when creating the job and the resulting custom model artifacts in an S3 bucket controlled by AWS. The output files are encrypted with the encryption configurations of the S3 bucket. These are [encrypted either with (Score: 6.448048114776611)\n",
      "\n",
      "\n",
      "your app should not expose fine tuning data in any form, then you should first filter out confidential data from your training data. If you already created a customized model using confidential data by mistake, you can delete that custom model, filter out confidential information from the training data, and then create a new model. For encrypting custom models (including copied models), Amazon Bedrock offers you two options: 1. AWS owned keys – By default, Amazon Bedrock encrypts custom models with AWS owned keys. You can't view, manage, or use AWS owned keys, or audit their use. However, you don't have to take any action or change any programs to protect the keys that encrypt your data. For more [information, see AWS owned keys in the AWS Key Management Service Developer Guide.](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-owned-cmk) 2. Customer managed keys – You can choose to encrypt custom models with customer managed [keys that you manage yourself. For (Score: 5.597530364990234)\n",
      "\n",
      "\n",
      "\"kms:ViaService\": [ \"bedrock.${region}.amazonaws.com\" --- Data encryption 1024 The roles that will customize the model and the roles that will invoke the model are different If the roles that will invoke the custom model are different from the role that will customize the model, you need both the statement from Encrypt a model and Allow access to an encrypted model. Modify the statements in the following policy template as follows: 1. The first statement allows encryption and decryption of the key. In the Principal field, add accounts that you want to allow to customize the custom model to the list that the AWS subfield maps to. 2. The second statement allows only decryption of the key. In the Principal field, add accounts that you want to only allow to invoke the custom model to the list that the AWS subfield maps to. --- { \"Version\": \"2012-10-17\", \"Id\": \"PermissionsCustomModelKey\", \"Statement\": [ { \"Sid\": \"PermissionsEncryptCustomModel\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ (Score: 5.168539524078369)\n",
      "\n",
      "\n",
      "the Amazon Bedrock service. Although you can only attach one key policy to a key, you can attach multiple statements to the key policy by adding staements to the list in the Statement field of the policy. The following statements are relevant to encrypting custom and copied models: Encrypt a model To use your customer managed key to encrypt a custom or copied model, include the following statement in a key policy to allow encryption of a model. In the Principal field, add accounts that you want to allow to encrypt and decrypt the key to the list that the AWS subfield maps to. If you use the kms:ViaService condition key, you can add a line for each region, or use * in place of ${region} to allow all regions that support Amazon Bedrock. --- { \"Sid\": \"PermissionsEncryptDecryptModel\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${account-id}:user/${role}\" --- Data encryption 1022 --- }, \"Action\": [ \"kms:Decrypt\", \"kms:GenerateDataKey\", \"kms:DescribeKey\", \"kms:CreateGrant\" ], (Score: 4.971851825714111)\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"model customization 작업에서 데이터 암호화하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "\n",
    "status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1908]\n",
      "Traversing 'Custom models'...\n",
      "target_node: [2027]\n",
      "Traversing 'Code samples for model customization'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 14\n",
      "==============\n",
      "Search Type: local_search\n",
      "==============\n",
      "file, replacing ${training-bucket} and ${output-bucket} with your S3 bucket names. --- { \"trainingDataConfig\": { \"s3Uri\": \"s3://${training-bucket}/train.jsonl\" }, \"outputDataConfig\": { \"s3Uri\": \"s3://${output-bucket}\" } } --- To submit a model customization job, navigate to the folder containing --- FineTuningData.json in a terminal and run the following command in the command line, --- replacing ${your-customization-role-arn} with the model customization role that you set up. --- aws bedrock create-model-customization-job \\ --customization-type FINE_TUNING \\ --base-model-identifier arn:aws:bedrock:us-east-1::foundation-model/ amazon.titan-text-express-v1 \\ --role-arn ${your-customization-role-arn} \\ --job-name MyFineTuningJob \\ --custom-model-name MyCustomModel \\ --hyper-parameters epochCount=1,batchSize=1,learningRate=.0005,learningRateWarmupSteps=0 \\ --cli-input-json file://FineTuningData.json --- The response returns a jobArn. Allow the job some time to complete. You can check its (Score: 7.889132499694824)\n",
      "\n",
      "\n",
      "created. To check its status, provide the name or ARN of the provisioned model as the --- provisioned-model-id in the following command. --- aws bedrock get-provisioned-model-throughput \\ --provisioned-model-id ${provisioned-model-arn} --- --- When the status is InService, you can run inference with your custom model with the following command. You must provide the ARN of the provisioned model as the model-id. The output is written to a file named output.txt in your current folder. --- aws bedrock-runtime invoke-model \\ --model-id ${provisioned-model-arn} \\ --body '{\"inputText\": \"What is AWS?\", \"textGenerationConfig\": {\"temperature\": 0.5}}' \\ --cli-binary-format raw-in-base64-out \\ output.txt --- Code samples 954 Python Run the following code snippet to submit a fine-tuning job. Replace ${your-customization--- role-arn} with the ARN of the MyCustomizationRole that you set up and replace ${training-bucket} and ${output-bucket} with your S3 bucket names. --- import boto3 import json (Score: 6.566826820373535)\n",
      "\n",
      "\n",
      "The following code samples show how to prepare a basic dataset, set up permissions, create a custom model, view the output files, purchase throughput for the model, and run inference on the model. You can modify these code snippets to your specific use-case. 1. Prepare the training dataset. a. Create a training dataset file containing the following one line and name it train.jsonl. --- {\"prompt\": \"what is AWS\", \"completion\": \"it's Amazon Web Services\"} --- b. Create an S3 bucket for your training data and another one for your output data (the names must be unique). c. Upload train.jsonl into the training data bucket. Code samples 947 2. Create a policy to access your training and attach it to an IAM role with a Amazon Bedrock trust relationship. Select the tab corresponding to your method of choice and follow the steps. Console 1. Create the S3 policy. a. [Navigate to the IAM console at https://console.aws.amazon.com/iam and choose](https://console.aws.amazon.com/iam) Policies from the (Score: 6.3483991622924805)\n",
      "\n",
      "\n",
      "bedrock.get_provisioned_model_throughput(provisionedModelId=provisionedModelArn) --- --- When the status is InService, you can run inference with your custom model with the following command. You must provide the ARN of the provisioned model as the modelId. --- import json import logging import boto3 from botocore.exceptions import ClientError class ImageError(Exception): \"Custom exception for errors returned by the model\" def __init__(self, message): self.message = message logger = logging.getLogger(__name__) --- Code samples 956 --- logging.basicConfig(level=logging.INFO) def generate_text(model_id, body): \"\"\" Generate text using your provisioned custom model. Args: model_id (str): The model ID to use. body (str) : The request body to use. Returns: response (json): The response from the model. \"\"\" logger.info( \"Generating text with your provisioned custom model %s\", model_id) brt = boto3.client(service_name='bedrock-runtime') accept = \"application/json\" content_type = (Score: 5.813394546508789)\n",
      "\n",
      "\n",
      "status with the following command. --- aws bedrock get-model-customization-job \\ --job-identifier \"jobArn\" --- Code samples 953 When the status is COMPLETE, you can see the trainingMetrics in the response. You can download the artifacts to the current folder by running the following command, replacing --- aet.et-bucket with your output bucket name and jobId with the ID of the customization --- job (the sequence following the last slash in the jobArn). --- aws s3 cp s3://${output-bucket}/model-customization-job-jobId . --recursive --- Purchase a no-commitment Provisioned Throughput for your custom model with the following command. Note You will be charged hourly for this purchase. Use the console to see price estimates for different options. --- aws bedrock create-provisioned-model-throughput \\ --model-id MyCustomModel \\ --provisioned-model-name MyProvisionedCustomModel \\ --model-units 1 --- The response returns a provisionedModelArn. Allow the Provisioned Throughput some time to be (Score: 5.7399396896362305)\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock의 Custom Model 활용 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "\n",
    "status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1359]\n",
      "Traversing 'Agents for Amazon Bedrock'...\n",
      "target_node: [1454]\n",
      "Traversing 'Use memory to retain conversational context across multiple sessions'...\n",
      "target_node: [1458]\n",
      "Traversing 'Configure memory for your Amazon Bedrock agent'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 4\n",
      "context: Configure memory for your Amazon Bedrock agent\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "To configure memory for your agent, you must first enable memory and then optionally specify [the retention period for the memory. You can enable memory for your agent when you create or](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-create.html) [update your agent.](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-manage.html#agents-edit) To learn how to configure memory for your agent, select the tab corresponding to your method of choice and follow steps. Console To configure memory for your agent 1. If you're not already in the agent builder, do the following: a. Sign in to the AWS Management Console using an IAM role with Amazon [Bedrock permissions, and open the Amazon Bedrock console at https://](https://console.aws.amazon.com/bedrock/) [console.aws.amazon.com/bedrock/.](https://console.aws.amazon.com/bedrock/) b. Select Agents from the left navigation pane. Then, choose an agent in the Agents section. c. Choose Edit in Agent Builder 2. In the Agent details section, for Select model, make sure to select either Claude 3 Sonnet or Claude 3 Haiku. 3. In the Memory section, do the following: a. Select Enabled. b. (Optional) By default, agent retains conversational context for 30 days. To configure a custom retention period, enter a number between 1 and 30 to specify the memory duration for your agent. 4. Make sure to first Save and then Prepare to apply the changes you have made to the agent before testing it. API [To enable and configure memory for your agent, send an CreateAgent or UpdateAgent request](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_CreateAgent.html) [with an Agents for Amazon Bedrock build-time endpoint.](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#bra-bt) Configure memory for Amazon Bedrock agent 692 In the Amazon Bedrock API, you specify the memoryConfiguration when you send a an [CreateAgent or UpdateAgent request.](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_CreateAgent.html) The following shows the general format of the memoryConfiguration: \"memoryConfiguration\": {\n",
      "\"enabledMemoryTypes\": [ \"SESSION_SUMMARY\" ],\n",
      "\"storageDays\":30\n",
      "},  \n",
      "---  \n",
      "You can optionally configure the memory retention period by assigning the storageDays with\n",
      "a number between 1 and 30 days.  \n",
      "Note  \n",
      "If you enable memory for the agent and do not specify memoryId when you invoke the\n",
      "agent, agent will not store that specific turn in the memory.\n",
      "==============\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock Agent에서 memory 기능을 활용하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"context:\", context.parent_name)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling_contents_dev(graph, parent_id, content, k=5):\n",
    "    content_query = \"\"\"\n",
    "        MATCH (n)-[:HAS_CONTENTS]->(c)\n",
    "        WHERE id(n) = $parent_id\n",
    "        RETURN c.text WHERE c.order >= $order_pos\n",
    "        ORDER BY c.order\n",
    "        LIMIT $k\n",
    "    \"\"\"       \n",
    "    trial = 1\n",
    "    order_pos = k * trial\n",
    "    params = {\"parent_id\": parent_id, \"k\": k, \"order_pos\": order_pos} \n",
    "\n",
    "    content_results = graph.run(content_query, params)\n",
    "    sibling_content = [record[\"c.text\"] for record in content_results]\n",
    "    contents = \" \".join([content, sibling_content])\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [1070]\n",
      "Traversing 'Knowledge bases for Amazon Bedrock'...\n",
      "target_node: [1277]\n",
      "Traversing 'Sync your data source with your Amazon Bedrock knowledge base'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 6\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "After you create your knowledge base, you ingest your data source/sources into your knowledge base so that they're indexed and are able to be queried. Ingestion converts the raw data in your data source into vector embeddings. Before you begin ingestion, check that your data source fulfills the following conditions: - You have configured the connection information for your data source. To configure a data source [connector to crawl your data from your data source repository, see Supported data source](https://docs.aws.amazon.com/bedrock/latest/userguide/data-source-connectors.html) [connectors.](https://docs.aws.amazon.com/bedrock/latest/userguide/data-source-connectors.html) [• The files are in supported formats. For more information, see Support document formats.](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-ds.html#kb-ds-supported-doc-formats-limits) - The files don't exceed the maximum file size of 50 MB. For more information, see Knowledge base quotas. - If your data source contains metadata files, check the following conditions to ensure that the metadata files aren't ignored: - Each .metadata.json file shares the same name as the source file that it's associated with. Sync your data source 600 - If the vector index for your knowledge base is in an Amazon OpenSearch Serverless vector store, check that the vector index is configured with the faiss engine. If the vector index is configured with the nmslib engine, you'll have to do one of the following: - Create a new knowledge base in the console and let Amazon Bedrock automatically create a vector index in Amazon OpenSearch Serverless for you. - Create another vector index in the vector store and select faiss as the Engine. Then create a new knowledge base and specify the new vector index. - If the vector index for your knowledge base is in an Amazon Aurora database cluster, check that the table for your index contains a column for each metadata property in your metadata files before starting ingestion. Note Each time you add, modify, or remove files from your data source, you must sync the data source so that it is re-indexed to the knowledge base. Syncing is incremental, so Amazon Bedrock only processes added, modified, or deleted documents since the last sync. To learn how to ingest your data sources into your knowledge base, Select the tab corresponding to your method of choice and follow the steps. Console To ingest your data sources 1. [Open the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/.](https://console.aws.amazon.com/bedrock/) 2. From the left navigation pane, select Knowledge base and choose your knowledge base. 3. In the Data source section, select Sync to begin data ingestion. 4. When data ingestion completes, a green success banner appears if it is successful. Note After data ingestion completes, it could take few minutes for the vector embeddings of the newly ingested data to be available in the vector store if you use Amazon OpenSearch Serverless. Sync your data source 601 5. You can choose a data source to view its Sync history. Select View warnings to see why a data ingestion job failed. API To ingest a data source into the vector store you configured for your knowledge base, send a [StartIngestionJob request with a Agents for Amazon Bedrock build-time endpoint. Specify the](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_StartIngestionJob.html) --- knowledgeBaseId and dataSourceId. --- [Use the ingestionJobId returned in the response in a GetIngestionJob request with a Agents](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_GetIngestionJob.html) [for Amazon Bedrock build-time endpoint to track the status of the ingestion job. In addition,](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#bra-bt) specify the knowledgeBaseId and dataSourceId. - When the ingestion job finishes, the status in the response is COMPLETE. Note After data syncing completes, it could take a few minutes for the vector embeddings of the newly synced data to reflect in your knowledge base if you use Amazon OpenSearch Serverless. - The statistics object in the response returns information about whether ingestion was successful or not for documents in the data source. You can also see information for all ingestion jobs for a data source by sending a [ListIngestionJobs request with a Agents for Amazon Bedrock build-time endpoint. Specify the](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_ListIngestionJobs.html) --- dataSourceId and the knowledgeBaseId of the knowledge base that the data is being --- ingested to. - Filter for results by specifying a status to search for in the filters object. - Sort by the time that the job was started or the status of a job by specifying the sortBy object. You can sort in ascending or descending order. - Set the maximum number of results to return in a response in the maxResults field. If there are more results than\n",
      "==============\n",
      "Partial\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Knowledgebase의 소스로 웹 페이지를 사용하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "\n",
    "status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [2672]\n",
      "Traversing 'Code examples for Amazon Bedrock using AWS SDKs'...\n",
      "target_node: [2728]\n",
      "Traversing 'Code examples for Amazon Bedrock Runtime using AWS SDKs'...\n",
      "target_node: [3004]\n",
      "Traversing 'Cohere Command for Amazon Bedrock Runtime using AWS SDKs'...\n",
      "target_node: [3018]\n",
      "Traversing 'Invoke Cohere Command on Amazon Bedrock using Bedrock's Converse API with a response stream'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 9\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "The following code examples show how to send a text message to Cohere Command, using Bedrock's Converse API and process the response stream in real-time. .NET AWS SDK for .NET Note There's more on GitHub. Find the complete example and learn how to set up and run [in the AWS Code Examples Repository.](https://github.com/awsdocs/aws-doc-sdk-examples/tree/main/dotnetv3/Bedrock-runtime#code-examples) Send a text message to Cohere Command, using Bedrock's Converse API and process the response stream in real-time. --- // Use the Converse API to send a text message to Cohere Command // and print the response stream. using System; using System.Collections.Generic; using System.Linq; using Amazon; using Amazon.BedrockRuntime; using Amazon.BedrockRuntime.Model; --- Cohere Command 1383 --- // Create a Bedrock Runtime client in the AWS Region you want to use. var client = new AmazonBedrockRuntimeClient(RegionEndpoint.USEast1); // Set the model ID, e.g., Command R. var modelId = \"cohere.command-r-v1:0\"; // Define the user message. var userMessage = \"Describe the purpose of a 'hello world' program in one line.\"; // Create a request with the model ID, the user message, and an inference configuration. var request = new ConverseStreamRequest { ModelId = modelId, Messages = new List<Message> { new Message { Role = ConversationRole.User, Content = new List<ContentBlock> { new ContentBlock { Text = userMessage } } } }, InferenceConfig = new InferenceConfiguration() { MaxTokens = 512, Temperature = 0.5F, TopP = 0.9F } }; try { // Send the request to the Bedrock Runtime and wait for the result. var response = await client.ConverseStreamAsync(request); // Extract and print the streamed response text in real-time. foreach (var chunk in response.Stream.AsEnumerable()) { if (chunk is ContentBlockDeltaEvent) { Console.Write((chunk as ContentBlockDeltaEvent).Delta.Text); } --- Cohere Command 1384 --- } } catch (AmazonBedrockRuntimeException e) { Console.WriteLine($\"ERROR: Can't invoke '{modelId}'. Reason: {e.Message}\"); throw; } --- [• For API details, see ConverseStream in AWS SDK for .NET API Reference.](https://docs.aws.amazon.com/goto/DotNetSDKV3/bedrock-runtime-2023-09-30/ConverseStream) Java SDK for Java 2.x Note There's more on GitHub. Find the complete example and learn how to set up and run [in the AWS Code Examples Repository.](https://github.com/awsdocs/aws-doc-sdk-examples/tree/main/javav2/example_code/bedrock-runtime#readme) Send a text message to Cohere Command, using Bedrock's Converse API and process the response stream in real-time. --- // Use the Converse API to send a text message to Cohere Command // and print the response stream. import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider; import software.amazon.awssdk.regions.Region; import software.amazon.awssdk.services.bedrockruntime.BedrockRuntimeAsyncClient; import software.amazon.awssdk.services.bedrockruntime.model.ContentBlock; import software.amazon.awssdk.services.bedrockruntime.model.ConversationRole; import software.amazon.awssdk.services.bedrockruntime.model.ConverseStreamResponseHandler; import software.amazon.awssdk.services.bedrockruntime.model.Message; import java.util.concurrent.ExecutionException; public class ConverseStream { public static void main(String[] args) { --- Cohere Command 1385 --- // Create a Bedrock Runtime client in the AWS Region you want to use. // Replace the DefaultCredentialsProvider with your preferred credentials provider. var client = BedrockRuntimeAsyncClient.builder() .credentialsProvider(DefaultCredentialsProvider.create()) .region(Region.US_EAST_1) .build(); // Set the model ID, e.g., Command R. var modelId = \"cohere.command-r-v1:0\"; // Create the input text and embed it in a message object with the user role. var inputText = \"Describe the purpose of a 'hello world' program in one line.\"; var message = Message.builder() .content(ContentBlock.fromText(inputText)) .role(ConversationRole.USER) .build(); // Create a handler to extract and print the response text in real-time. var responseStreamHandler = ConverseStreamResponseHandler.builder() .subscriber(ConverseStreamResponseHandler.Visitor.builder() .onContentBlockDelta(chunk -> { String responseText = chunk.delta().text(); System.out.print(responseText); }).build() ).onError(err -> System.err.printf(\"Can't invoke '%s': %s\", modelId, err.getMessage()) ).build(); try { // Send the message with a basic inference configuration and attach the handler. client.converseStream(request -> request.modelId(modelId) .messages(message) .inferenceConfig(config -> config .maxTokens(512) .temperature(0.5F) .topP(0.9F) ), responseStreamHandler).get(); --- Cohere Command 1386 --- } catch (ExecutionException | InterruptedException e) { System.err.printf(\"Can't invoke '%s': %s\", modelId, e.getCause().getMessage()); } } } --- [• For API details, see ConverseStream in AWS SDK for Java 2.x API\n",
      "==============\n",
      "Partial\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 \n",
    "question = \"Bedrock에서 Converse API를 활용할 때 응답 양식\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "\n",
    "status = check_relevance_dev(question, search_content, context.parent_name, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon Bedrock\n",
      "[0] | Amazon Bedrock | traverse_child\n",
      "target_node: [0]\n",
      "Traversing 'Amazon Bedrock'...\n",
      "target_node: [3333]\n",
      "Traversing 'Quotas for Amazon Bedrock'...\n",
      "target_node: [3346]\n",
      "Traversing 'Model inference prompt quotas'...\n",
      "No child. Proceed to 'get_contents'...\n",
      "Num Documents: 2\n",
      "==============\n",
      "Search Type: get_short_documents\n",
      "==============\n",
      "Select a tab to see model-specific quotas for prompts. Amazon Titan Text models |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text prompt length, in characters|42,000|No| Amazon Titan Image Generator G1 V1 |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text prompt length, in characters|1,024|No| Model inference prompt quotas 1574 |Description|Value|Adjustable through Service Quotas| |---|---|---| |Input image size|5 MB|No| |Input image height in pixels (inpainting/outpainting)|1,024|No| |Input image width in pixels (inpainting/outpainting)|1,024|No| |Input image height in pixels (image variation)|4,096|No| |Input image width in pixels (image variation)|4,096|No| |Input image total pixels|12,582,912|No| Amazon Titan Embeddings G1 - Text |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text input length, in characters|50,000|No| Amazon Titan Multimodal Embeddings G1 |Description|Value|Adjustable through Service Quotas| |---|---|---| |Text input length, in characters|100,000|No| |Base64-encoded string of image, in characters|25,000,000|No| Model inference prompt quotas 1575\n",
      "==============\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 질문의 주제 선택 (찾을 수 없는 정보)\n",
    "question = \"Bedrock의 가격 정책\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(graph, result.parent_id, 5)\n",
    "print(\"==============\")\n",
    "print(\"Search Type:\", context.search_type)\n",
    "if context.search_type == 'local_search':\n",
    "    search_content = local_search_dev(question, graph, context.parent_id, context.parent_name, context.k)\n",
    "    print(\"==============\")\n",
    "    print(search_content)\n",
    "    print(\"==============\")\n",
    "else:\n",
    "    print(\"==============\")\n",
    "    print(context.contents)\n",
    "    print(\"==============\")\n",
    "\n",
    "status = check_relevance_dev(question, search_content, context.contents_length, context.search_type, context.k)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate_answer_dev(question, context):\n",
    "    # Prompt setting\n",
    "    sys_prompt_template = \"당신은 AWS에 정통한 전문 엔지니어입니다. 주어진 사전 정보만 활용하여, 사용자 질문에 답변을 생성하세요. 사전 정보로 주어지지 않은 내용에 대한 질문에는 모른다고 답변하세요.\"\n",
    "    usr_prompt_template = \"#사전 정보: {context}\\n\\n #사용자 질문:\\n {question}\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", sys_prompt_template), (\"human\",usr_prompt_template)])\n",
    "\n",
    "    # Model setting\n",
    "    model_kwargs = {\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    llm = ChatBedrock(model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", region_name=\"us-west-2\", model_kwargs=model_kwargs, streaming=True)   \n",
    "\n",
    "    # Output setting\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | parser\n",
    "    for chunk in chain.stream({\"context\": context, \"question\": question}):\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Agents 기능의 성능을 최적화 하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(question, result.parent_id, 5)\n",
    "print(\"Status:\", context.status)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"Bedrock에서 Knowledge Base를 테스트하는 방법\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(question, result.parent_id, 5)\n",
    "print(\"Status:\", context.status)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택 (문서에 없는 정보)\n",
    "question = \"Bedrock의 가격 정책\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(question, result.parent_id, 5)\n",
    "print(\"Status:\", context.status)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling_contents_dev(cur_pos, order_pos=5):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문의 주제 선택\n",
    "question = \"What is Amazon Bedrock Playground?\"\n",
    "target_node, subgraph, next_step = select_subgraph_dev(question, graph)\n",
    "print(target_node, \"|\", subgraph, \"|\", next_step)\n",
    "\n",
    "# 적합한 헤더 찾기\n",
    "while target_node:\n",
    "    print(\"target_node:\", target_node)\n",
    "    result = traverse_child_dev(question, subgraph, graph, target_node)\n",
    "    traverse_results.append(result)\n",
    "\n",
    "    if result.next_action == \"get_content\":\n",
    "        break\n",
    "\n",
    "    if result.selected_child_ids:\n",
    "        target_node = [result.selected_child_ids[0]]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 문서 얻어내기\n",
    "context = get_contents_dev(question, result.parent_id, 5)\n",
    "print(\"Status:\", context.status)\n",
    "print(\"==============\")\n",
    "print(context.contents)\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
